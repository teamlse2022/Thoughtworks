{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e1413db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Note: Please ensure you run this workbook from the same folder as where the supplied data files are stored in your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc98439",
   "metadata": {},
   "source": [
    "# Preparing the Environment for Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb69f50b",
   "metadata": {},
   "source": [
    "In order for the work to be reproduceable, before running this notebook, please ensure that the requisite libraries/modules are installed on the local machine. In particular, amongst some of the libraries that are being used here which were not covered in the course can be installed by running the pip install cell below.\n",
    "\n",
    "There may be other libraries or modules are being used here which will need to be installed locally. Please go through full list of models and libraries to ensure that the most recent versions of all of them are installed.\n",
    "\n",
    "It is possible to **skip this section in part or whole if some or all the libraries listed below in the pip install command cell are already installed** on the local machine. \n",
    "\n",
    "Additionally, please ensure that all the datafiles supplied with this notebook and the underlying raw data from TfL supplied by LSE/TW are located in the same directory as where this notebook is saved on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cbf26b",
   "metadata": {},
   "source": [
    "### Coding Methodology\n",
    "\n",
    "Variable (Dataframe) names, such as df, df1..., have been reused in most of the cases to keep memory and resource allocation in this jupyter notebook to a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4986c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing additional libraries.\n",
    "# It is possible that this take some time depending on your local machine and internet connection.\n",
    "!pip install calmap\n",
    "!pip install gapminder\n",
    "!pip install plotly_calplot\n",
    "!pip install ipywidgets\n",
    "!pip install regex\n",
    "!pip install pywaffle\n",
    "!pip install calplot\n",
    "!pip install plotly\n",
    "!pip install --upgrade autopep8\n",
    "!pip install pycodestyle\n",
    "!pip install --upgrade pycodestyle\n",
    "!pip uninstall pycodestyle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b427e8e8",
   "metadata": {},
   "source": [
    "### Datafiles supplied with this Notebook\n",
    "\n",
    "**Twitter scrapes:**\n",
    "\n",
    "- twitter_scrape_071022en.csv\n",
    "- twitter_scrape_081022en.csv\n",
    "- twitter_scrape_091022en.csv\n",
    "- twitter_scrape_101022en.csv\n",
    "- twitter_scrape_111022en.csv\n",
    "- twitter_scrape_111022ena.csv\n",
    "- twitter_scrape_121022en.csv\n",
    "- twitter_scrape_131022en.csv\n",
    "- twitter_scrape_131022ena.csv\n",
    "- twitter_scrape_131022enb.csv\n",
    "- twitter_scrape_131022enc.csv\n",
    "- twitter_scrape_141022en.csv\n",
    "- twitter_scrape_151022en.csv\n",
    "- twitter_scrape_151022enb.csv\n",
    "- twitter_scrape_161022en.csv\n",
    "- twitter_scrape_161022ena.csv\n",
    "- twitter_scrape_171022en.csv\n",
    "- twitter_scrape_191022en.csv\n",
    "- user.csv\n",
    "\n",
    "**Other External Data:**\n",
    "- London_population.csv\n",
    "- New_York_population.csv\n",
    "- Sydney_population.csv\n",
    "- private_cars_london.csv\n",
    "- traffic_flow_borough.csv\n",
    "- London_Biking_sites_reconv.xlsx - This file contains the same information as the supplied data on London's cycle parking sites except that the \"Eastings\" & \"Northings\" data has been converted into the more conventional \"Longitude\" & \"Latitude\".\n",
    "- cycle_parking.csv\n",
    "- restricted_point.csv\n",
    "- signage.csv\n",
    "- signal.csv\n",
    "- traffic_calming.csv\n",
    "- advanced_stop_line.csv\n",
    "- restricted_route.csv\n",
    "- crossing.csv\n",
    "- cycle_lane_track.csv\n",
    "- TFL Cycle Hire 2017.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082894ee",
   "metadata": {},
   "source": [
    "**Underlying CSV Datafiles provided by LSE/ThoughtWorks are required but not supplied with this Notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67995b8",
   "metadata": {},
   "source": [
    "## Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76280001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries & set up date parsing functionality.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm \n",
    "import nltk\n",
    "import os\n",
    "import string   \n",
    "import re\n",
    "import math\n",
    "import squarify\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import functools as ft\n",
    "import calmap\n",
    "import calplot\n",
    "import locale\n",
    "\n",
    "# Copy your user specific YAML file and Twitter keys over to the same folder,\n",
    "# as the location of this Jupyter Notebook before you start to work.\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from twitter import *\n",
    "\n",
    "# Import modules.\n",
    "from gapminder import gapminder\n",
    "from pywaffle import Waffle\n",
    "from datetime import datetime\n",
    "from statsmodels.formula.api import ols\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from plotly_calplot import calplot\n",
    "from scipy.ndimage import gaussian_gradient_magnitude\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from collections import Counter\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setting up a date parser using a private funciton, lambda.\n",
    "# This will convert dates into a format required for aggregation & indexation.\n",
    "d_parser = lambda x: pd.datetime.strptime(x, '%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "# Default settings for matplotlibs to be used throughout\n",
    "plt.rcParams[\"axes.facecolor\"] = \"w\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3390ed3e",
   "metadata": {},
   "source": [
    "# Setting up locale for Mac Users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b18a6",
   "metadata": {},
   "source": [
    "Run this code only if the local machine is a Mac. Skip this if the machine is Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b0bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local setting for datetime parser to GB.\n",
    "locale.setlocale(locale.LC_ALL, 'en_GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8189f3",
   "metadata": {},
   "source": [
    "# Exploring Twitter Data about Cycling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7582fe68",
   "metadata": {},
   "source": [
    "Is it at all worthwhile to look at cycling in London? Asides the traditional data that has been supplied, the task here is to extract the most recent posts on Twitter about Cycling. From these, the objectives are:\n",
    "- Discover locational information around where the topic generates the most social media content from \n",
    "- To see if any of the three cities being analysed are amongst these locations\n",
    "- Get an overview of the sentiment expressed in such content on the topic\n",
    "- Get an overview of the most common words used in such content to discover any underlying themes that can be explored further.\n",
    "\n",
    "The output of this analysis using social data can be used as supplementary evidence to partially inform the hypothesis that Londoner's can be influenced to increase the uptake of cycling amongst them. \n",
    "\n",
    "**Note** some of the outputs in this section will change very slightly versus the notes as the underlying data will be marginally refreshed each time the work book is run. This cannot be accounted for in the analysis and accompanying insights. However, it is not expected that either the analysis or insights will change significantly on the basis of this marginal refresh of the underlying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970fe06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the YAML file and Twitter keys over to this Jupyter Notebook before you start to work.\n",
    "# Import the yaml file - remember to specify the whole path and use / between directories.\n",
    "# The name of your yaml file will need to be inserted.\n",
    "# Here the yaml file on the local machine is called twitter.yaml.\n",
    "twitter_creds = yaml.safe_load(open('twitter.yaml', 'r').read()) \n",
    "\n",
    "\n",
    "# To investigate the Tweets & Sentiment Analysis\n",
    "# ensure that the latest packages are installed.\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass your Twitter credentials.\n",
    "twitter_api = Twitter(auth=OAuth(twitter_creds['access_token'],\n",
    "                                 twitter_creds['access_token_secret'], \n",
    "                                 twitter_creds['api_key'],\n",
    "                                 twitter_creds['api_secret_key'] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc719323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Twitter connection.\n",
    "print(twitter_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ef82c9",
   "metadata": {},
   "source": [
    "## Building up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd1d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for the term cycling on Twitter.\n",
    "# Ordered by recency.\n",
    "# Filtering for Tweets only in English.\n",
    "# Note that the free Twitter API being used here only allows access to the seven most recent\n",
    "# days of tweets.\n",
    "# Looks for the most recent 4,000 tweets on cycling each time this is run.\n",
    "\n",
    "q = {'q':'cycling', 'lang':'en', 'count':100, 'result_type':'recent'}\n",
    "\n",
    "# Results as an empty list.\n",
    "results = []\n",
    "\n",
    "while len(results) < 40:\n",
    "    query = twitter_api.search.tweets(**q)\n",
    "    q['max_id'] = query['search_metadata']['next_results'].split('&')[0].split('?max_id=')[1]\n",
    "    results.append(query)\n",
    "    \n",
    "# Determine the number of results.\n",
    "len(results)\n",
    "\n",
    "# Repeat this process over several days to get a meaningfully large dataset.\n",
    "# Continue this once a day for as long as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e75cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the results in a DataFrame for NLP Analysis.\n",
    "df = pd.concat([pd.DataFrame(_['statuses']) for _ in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data as a backup & in order to build up the data set.\n",
    "# Each scrape only gathers max 4000 tweets.\n",
    "# So, to build a large enough unique dataset, have to scrape over several days\n",
    "df.to_csv('twitter_scrape_191022en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a2607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up the data source from the daily extracts.\n",
    "df2 = pd.read_csv('twitter_scrape_071022en.csv')\n",
    "df3 = pd.read_csv('twitter_scrape_081022en.csv')\n",
    "df4 = pd.read_csv('twitter_scrape_091022en.csv')\n",
    "df5 = pd.read_csv('twitter_scrape_101022en.csv')\n",
    "df6 = pd.read_csv('twitter_scrape_111022en.csv')\n",
    "df7 = pd.read_csv('twitter_scrape_111022ena.csv')\n",
    "df8 = pd.read_csv('twitter_scrape_121022en.csv')\n",
    "df9 = pd.read_csv('twitter_scrape_131022en.csv')\n",
    "df10 = pd.read_csv('twitter_scrape_131022ena.csv')\n",
    "df11 = pd.read_csv('twitter_scrape_131022enb.csv')\n",
    "df12 = pd.read_csv('twitter_scrape_131022enc.csv')\n",
    "df13 = pd.read_csv('twitter_scrape_141022en.csv')\n",
    "df14 = pd.read_csv('twitter_scrape_151022en.csv')\n",
    "df15 = pd.read_csv('twitter_scrape_151022enb.csv')\n",
    "df16 = pd.read_csv('twitter_scrape_161022en.csv')\n",
    "df17 = pd.read_csv('twitter_scrape_161022ena.csv')\n",
    "df18 = pd.read_csv('twitter_scrape_171022en.csv')\n",
    "df19 = pd.read_csv('twitter_scrape_191022en.csv')\n",
    "\n",
    "# c.71,240 tweets have been extracted over the past 12 days.\n",
    "# Provides a meaningfully large dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df96c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a master dataset by concatenating the datasets.\n",
    "Df = pd.concat([df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e79f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View random samples of the outputs.\n",
    "Df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6425c889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Metadata.\n",
    "Df.info()\n",
    "\n",
    "# 71,244 tweets so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e4ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the created_at column into pd datetime format.\n",
    "Df['created_at'] = pd.to_datetime(Df.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e80808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for duplicates & retain just the most recent tweet.\n",
    "# Look for duplicates and drop them based on created_at and user.\n",
    "\n",
    "df = Df.drop_duplicates(subset = ['created_at', 'user', 'id'],\n",
    "  keep = 'last').reset_index(drop = True)\n",
    "\n",
    "# This avoids duplication of tweets created by the same user, at the same time & same id.\n",
    "# Duplicates may have come through as part of the scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee4f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Metadata.\n",
    "df.info()\n",
    "\n",
    "# Not much geo data captured. \n",
    "# User has geo data within it after keyword 'location' per Twitter API documentation.\n",
    "# 64,661 unique tweets extracted on cycling so far.\n",
    "# Need to keep running this process every day to build up a meaningfully large database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract user details into a seperate dataframe for later analysis.\n",
    "# Twitter Api docs indicate this field may contain location data.\n",
    "df1 = df['user'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13199349",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View one raw result.\n",
    "df1[2]\n",
    "\n",
    "# Confirms user has many variables incl location data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f58dc8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confirm how the data under user is presently stored.\n",
    "print(type(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f784a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimension of numpy array.\n",
    "np.shape(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294020cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas dataframe.\n",
    "user = pd.DataFrame(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check output.\n",
    "user.info()\n",
    "# 1 Column with 64,661 rows of data. \n",
    "# Need to transform this where rows become columns.\n",
    "# user id is stripped of its underlying metadata\n",
    "# as this is still a small enough file, will do this in Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385a30b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert to Excel file for back & re-import later when analysing location data.\n",
    "user.to_csv('user.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17e6fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the text of the actual tweet\n",
    "df2 = df['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b1102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View one raw result.\n",
    "df2[9]\n",
    "\n",
    "# Confirms that the body of the post is stored here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c6eac",
   "metadata": {},
   "source": [
    "## Prepare the data for NLP & Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd787ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce Stopwords.\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9355d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each tweet into individual words.\n",
    "df2_token = [word_tokenize(_) for _ in df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a2aea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of English words to exclude words that don't appear on the list.\n",
    "all_english_words = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad1ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some pre-processing:\n",
    "# Get every word.\n",
    "# Convert it to lowercase.\n",
    "# Only include if the word is alphanumeric and if it is in the list of English words.\n",
    "\n",
    "df2_token_nostop =\\\n",
    "[[y.lower() for y in x if y.lower() not in stop_words and y.isalpha() and y.lower() in all_english_words]\\\n",
    " for x in df2_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb55c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable to store the Sentiment Intensity Analyser.\n",
    "darth_vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279dd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through a dictionary comprehension to take every cleaned tweet. \n",
    "# Next run the polarity score function on the strings.\n",
    "# This will return four values in a dictionary.\n",
    "\n",
    "df2_polarity =\\\n",
    "{\" \".join(_) : darth_vader.polarity_scores(\" \".join(_)) for _ in df2_token_nostop}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d0548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionary results to a pandas dataframe. \n",
    "# The index is the cleaned tweet.\n",
    "\n",
    "polarity_pd = pd.DataFrame(df2_polarity).T\n",
    "\n",
    "# View the dataframe.\n",
    "polarity_pd\n",
    "\n",
    "# Compound score indicates actual sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b0a552",
   "metadata": {},
   "source": [
    "## Visualising the data with Charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8bdd9c",
   "metadata": {},
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28432446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the output in a distribution.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_plot = polarity_pd.reset_index()['compound'].sort_values()\n",
    "ax1 = plt.axes()\n",
    "_plot.plot(kind='bar')\n",
    "\n",
    "x_axis = ax1.axes.get_xaxis()\n",
    "x_axis.set_visible(False)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5ad3f3",
   "metadata": {},
   "source": [
    "- Lots of values are 0 (neutral) - blank spaces.\n",
    "- More positive sentiment than negative sentiment amongst non-neutral values.\n",
    "- Some very strong positive sentiment > 0.75.\n",
    "- Some very strong negative sentiment also visible < -0.75.\n",
    "\n",
    "A histogram plot will visualise the distribution of sentiment better. The strictly neutral values should be removed to help make the histogram clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab55de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove polarity values equal to zero whch are neutral sentiments.\n",
    "# This will improve the scale of the histogram and remove all strictly \n",
    "# neutral reviews from the analysis.\n",
    "# This will better highlight the distribution of polarity values = non-neutral sentiment.\n",
    "# Define the source of the data to use for plotting histogram.\n",
    "data = polarity_pd['compound'][polarity_pd['compound'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2100cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the distribution of the sentiment analysis using a histogram.\n",
    "count, edges, bars = plt.hist(data)\n",
    "plt.bar_label(bars)\n",
    "plt.title('Non Neutral Sentiments from most recent Tweets on Cycling')\n",
    "plt.show()\n",
    "\n",
    "# The exact numbers will change each time this notebook is rerun as the data will be changed.\n",
    "# Refresh the analytical content as this changes.\n",
    "\n",
    "# Many more strongly positive sentiment (>0.75 polarity score) = 1350 tweets.\n",
    "# Far fewer strongly negative sentiment (<-0.75 polarity score) = 190 tweets.\n",
    "# Overall, sentiment when non neutral is strongly skewed in favour of cycling.\n",
    "# Indicates that on the whole from this small sample size, sentiment towards cycling is positive.\n",
    "# However, recall there are c.64,660 unique tweets.\n",
    "# Only c.25% of tweets display non 0 sentiment (which are shown in chart below).\n",
    "# Means vast majority feel very neutral about cycling(shown in chart above).\n",
    "# Indicates that there is room to improve sentiment towards cycling.\n",
    "# This can only come from increased cycling uptake and campaign to increase cycling uptake.\n",
    "# Worthwhile therefore to pursue increasing uptake of cycling.\n",
    "# Scope to increase cycling uptake established using sentiment analysis given.\n",
    "# Scope to increase non neutral sentiment on cycling.\n",
    "# General non neutral sentiment is positive so people should have no objection\n",
    "# to at least trying out cycling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0130171",
   "metadata": {},
   "source": [
    "### Top Words Visualised in a WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3478a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all words are stored in a list to be used to create a wordcloud.\n",
    "print(type(df2_token_nostop))\n",
    "\n",
    "# Confirmed words are in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5286fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat list into string for use to generate wordcloud.\n",
    "s = ''.join(str(x) for x in df2_token_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the most common items in the list.\n",
    "# Will identify the obvious words to be removed from the list before wordcloud is generated.\n",
    "# This will enhance the accuracy of the wordcloud.\n",
    "from collections import Counter\n",
    "\n",
    "# split() returns list of all the words in the string.\n",
    "split_y = s.split()\n",
    "  \n",
    "# Pass the split_it list to instance of Counter class.\n",
    "Counter = Counter(split_y)\n",
    "  \n",
    "# most_common() produces k frequently encountered.\n",
    "# Input values and their respective counts.\n",
    "most_occur = Counter.most_common(10)\n",
    "  \n",
    "print(most_occur)\n",
    "\n",
    "# From these identify the most common words which fail to convey any meaning or sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803f7589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove cycling and other obvious words from the list \n",
    "# else they will skew the results.\n",
    "# Define these words as bespoke stopwords.\n",
    "stop_words = ['cycling', 'cycling ', ' cycling', ' cycling ', 'distance', 'distance ', \n",
    "              'cyclingcycling', 'transport', 'found', 'evidence', 'transport ', 'bike ', \n",
    "              'helmet', 'cycle ', 'transport ', 'public', 'society', 'source', 'hour', 'get']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866b6d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new list to avoid messing up orginal list.\n",
    "new_s = [word for word in s if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cfa29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformating into string again for wordcloud generation.\n",
    "st = ''.join(str(x) for x in new_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24944b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip all punctuation.\n",
    "new_string = st.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d648dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Word Cloud using the 30 most frequently used words\n",
    "# when tweeting about cycling.\n",
    "wordcloud = (WordCloud(max_words=30, \n",
    "                       background_color=\"white\").generate(new_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5848e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review: Display the wordcloud.\n",
    "wordcloud.generate(new_string)\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.axis('off') \n",
    "plt.imshow(wordcloud, interpolation=\"lanczos\")\n",
    "\n",
    "# Displays the top 25 most commonly used words by Twitter users when they post about cycling.\n",
    "# Relative Size displays frequency of the words.\n",
    "# There are words which suggest an ongoing debate about the potential merit of cycling.\n",
    "# Words such time, transport, suggest, spend and insane occur within the top 50 words.\n",
    "# These together, suggest evidence of a public who are still debating the merits of cycling.\n",
    "# Reinforces the case for the trying to influence cycling uptake as any debate\\\n",
    "# can within reason be influenced with mix of policy and strategic marketing\\\n",
    "# esp on social media. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b7c38a",
   "metadata": {},
   "source": [
    "## Insights from Geo Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd224f75",
   "metadata": {},
   "source": [
    "Objective is to analyse the location of Twitter users posting about cycling. The top most amongst should be the places of most interest to use for comparative analysis against London. \n",
    "\n",
    "Further, if London is in the list of places from where people most frequently tweet on cycling, it would show that Londoners are generally animated on the topic and post on the topic on social media. Therefore, it would be worthwhile to try to influence their uptake on cycling through a social media specific strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reintroduce the data on users which contains locational data.\n",
    "colnames=['user_info']\n",
    "user = pd.read_csv('user.csv', names=colnames, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa66d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the user_info column into its constituent parts.\n",
    "user = user['user_info'].str.split(',', expand=True)\n",
    "\n",
    "# Drop where index values are NaN.\n",
    "user = user[user.index.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6947b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View random sample from dataframe.\n",
    "user.sample()\n",
    "# Cols 4 and 5 are of interest as they contain location information.\n",
    "# Col 5 does not always contain location information.\n",
    "# Has to be included nevertheless to get the data when it does contain location information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87876dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming cols of interest.\n",
    "user.rename(columns = {user.columns[4]: 'city'}, inplace = True)\n",
    "user.rename(columns = {user.columns[5]: 'country'}, inplace = True)\n",
    "\n",
    "# Retaining just the cols of interest\n",
    "user = user[['city', 'country']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ac016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all punctuation marks from location columns.\n",
    "user[\"city\"] = user['city'].str.replace('[^\\w\\s]','')\n",
    "user[\"country\"] = user['country'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Remove all non-alphabet values & cast as string.\n",
    "user[\"city\"] = user['city'].astype(str).replace('[^a-zA-Z0-9 ]', '', regex=True)\n",
    "user[\"country\"] = user['country'].astype(str).replace('[^a-zA-Z0-9 ]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6997687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first two words from the column location.\n",
    "user['city'] = user['city'].str.split(n=1).str[1]\n",
    "\n",
    "# Delete all except first word in location 2.\n",
    "# location 2 appears to sometimes contain additional location value in the first word.\n",
    "user['country'] =user['country'].drop_duplicates().str.split().str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f1dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows with Null values in location.\n",
    "# As this is primary search objective, null value rows cannot be interpreted for analysis.\n",
    "# Removing null value rows will produce cleaner output.\n",
    "user = user[user.city.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4dbdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all Null values in location_2 with Unknown.\n",
    "\n",
    "user.country = user.country.fillna('Unknown')\n",
    "\n",
    "# Replace description with unknown in location_2.\n",
    "# Replace at home with unknown in location.\n",
    "user = user.replace(['description', 'At home'],['Unknown', 'Unknown'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee17bafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values in each column.\n",
    "for col in user:\n",
    "  print(col,\": \", user[col].nunique())\n",
    "\n",
    "# >11,000 unique cities\n",
    "# >1,200 unqiue fields in country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07478fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the data to ensure mapping is more accurate.\n",
    "# Aggregating some of the more obvious locations to London\\\n",
    "# incl home counties & general country descriptions.\n",
    "user['city'] = user['city'].replace(['UK','United Kingdom', 'England', 'NW London', 'N London', 'NE London',\n",
    "                                    'W London', 'E London', 'S London', 'SE London', 'SW London', 'Berkshire',\n",
    "                                    'Berks', 'Buckinghamshire', 'Bucks', 'Essex', 'Herts', 'Hertfordshire',\n",
    "                                    'Kent', 'Surrey', 'Sussex', 'Brixton', 'SE5'],'London')\n",
    "\n",
    "# Assumes all UK related locations listed above are London unless specified otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb46665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the city & country counts.\n",
    "locs = user['city'].value_counts()\n",
    "locs_2 = user['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f419b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View output from country values.\n",
    "locs.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e8bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Earth from the series as it cannot be assigned to a specific location.\n",
    "locs=locs.drop('Earth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b96a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View output from country values.\n",
    "locs_2.head(20)\n",
    "\n",
    "# Less useful as top 3 are clearly unidentifiable locations.\n",
    "# Before joining this data set with other location data\\\n",
    "# remove the clearly unidentifiable elements.\n",
    "# Then join the two series to aggregate the counts of the locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56fb86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first 3 elements as they are clearly not identifiable locations.\n",
    "locs_2 = locs_2.iloc[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aeb72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the 2 series vertically to capture all the location data from both series'.\n",
    "user_loc = pd.concat([locs_2, locs], axis=0)\n",
    "\n",
    "# Create a dataframe to analyse the data better.\n",
    "user_locs = pd.DataFrame({'location':user_loc.index, 'count':user_loc.values})\n",
    "\n",
    "# Agregate the locational data ahead of visualisation.\n",
    "user_locs= user_locs.groupby(['location'], as_index=False)['count'].sum()\n",
    "\n",
    "# Extract the 20 most popular locations by count.\n",
    "user_locs=user_locs.sort_values(by=['count']).tail(20)\n",
    "\n",
    "# Ensure datatypes are correctly classified.\n",
    "user_locs['location'] = user_locs['location'].astype(pd.StringDtype())\n",
    "\n",
    "# Returned an aggregated list of the top 20 locations from all the tweet user location data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60776a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the top 20 places where people are located when tweeting about cycling\n",
    "user_locs.plot.barh(x=\"location\", y=\"count\", title=\"Most Popular locations for tweeting on cycling\", \n",
    "                   figsize=(8,5), legend=False, fontsize=12, xlabel=' ' )\n",
    "\n",
    "# People from c.11,000 unique places are tweeting on cycling.\n",
    "# Can see that out of c.64,000 tweets examined, the most (3, 400+)\\\n",
    "# from a single location are from London.\n",
    "# c.5% of all people tweeting about cycling globally are readily identifiable as being from London.\n",
    "# This is very significant share for one city & also vastly outstrips the second most popular location.\n",
    "# The second most popular & clearly identifiable location is California (c.600 people)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d469b62a",
   "metadata": {},
   "source": [
    "Following feedback from the lo-fi presentation to ThoughtWorks, this area of analysis is being dropped. Whilst insights were generated, feedback from TW was that they werent sufficiently compelling or actionable. As such this area is being dropped although the key highlights are as follows:\n",
    "\n",
    "- Londoners tweet the most in the world on cycling\n",
    "- When combined with other locations within the country, UK is the overwhelmingly most popular location \n",
    "- WordCloud suggests lack of clarity on the usefulness of cycling\n",
    "- WordCloud identifed some key topics of interest\n",
    "- Polarity analysis suggests the existence of a large number of neutrals who have the potential to be influenced\n",
    "\n",
    "Together, the data suggests that there should be a social media campaign crafted by the client to engage Londoners on the key areas of interest to increase cycling uptake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f367b7aa",
   "metadata": {},
   "source": [
    "# Exploring supplied Data for insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c674ab2",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d5011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file(s) for count data only.\n",
    "ny_counts = pd.read_csv('Bicycle_Counts.csv', parse_dates=['date'], date_parser=d_parser)\n",
    "inner_london = pd.read_csv('Inner London.csv')\n",
    "central_london = pd.read_csv('Central London.csv')\n",
    "outer_london = pd.read_csv('Outer London.csv')\n",
    "sydney_counts = pd.read_csv('Bicycle_count_surveys.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646b402",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424db8fc",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a2b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the dataframe, metadata, shape.\n",
    "# Clearly segreate the information for each file.\n",
    "print(\"-----------NY Metadata & Info----------->\")\n",
    "ny_counts.info()\n",
    "print(\"-----------Inner London Metadata & Info----------->\")\n",
    "inner_london.info()\n",
    "print(\"-----------Central London Metadata & Info----------->\")\n",
    "central_london.info()\n",
    "print(\"-----------Outer London Metadata & Info----------->\")\n",
    "outer_london.info()\n",
    "print(\"-----------Sydney Metadata & Info----------->\")\n",
    "sydney_counts.info()\n",
    "\n",
    "# New York Dataset\n",
    "# ny_counts is a large but simple dateframe containing the number of bicycles passing a counters.\n",
    "# No missing data in ny_counts.  \n",
    "\n",
    "# London Datasets\n",
    "# Non-Null values for London dataframes across all columns dont add up. \n",
    "# Implies missing data.\n",
    "# Will need to explore further.\n",
    "# Central London is another very large datafreame.\n",
    "# Large dataframes need to be trimmed for unncessary data to reduce strain on memory use.\n",
    "\n",
    "# Sydney Dataset\n",
    "# Sydney observations are just aggregated by month and Site ID.\n",
    "# Sydney observations run for selected hours and not all day (looks like peak hours only).\n",
    "# Sydney also has no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78435958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get more precise handle on missing values in each dataframe.\n",
    "inner_london.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b38972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get more precise handle on missing values in each dataframe.\n",
    "central_london.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae33a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get more precise handle on missing values in each dataframe.\n",
    "outer_london.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8636f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at samples of the data.\n",
    "ny_counts.head(5)\n",
    "\n",
    "# Very sparse but clean data.\n",
    "# Id is site Id for where the counter is located.\n",
    "# Data runs until mid June 2022. \n",
    "# So data is very recent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2689931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring why NY data is so large.\n",
    "# View Tail to see end date.\n",
    "ny_counts.tail(5)\n",
    "\n",
    "# Data starts from mid Dec 2012.\n",
    "# Need to explore how far the other data sets run until."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b7c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at samples of the data.\n",
    "inner_london.sample(5)\n",
    "\n",
    "# Date has french word in it. Needs to cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6257e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at samples of the data.\n",
    "central_london.sample(5)\n",
    "\n",
    "# Data similar in format to inner london but has some extra columns.\n",
    "# Will need to trim this dataframe to concatenate.\n",
    "# Explore whether we need the extra columns and if not will trim.\n",
    "# Date has french word in it. Needs to cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8b881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at samples of the data.\n",
    "outer_london.head(5)\n",
    "\n",
    "# Matches format of inner london.\n",
    "# Date has french word in it. Needs to cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2803d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at samples of the data.\n",
    "sydney_counts.head(5)\n",
    "\n",
    "# Very sparse data.\n",
    "# Counts from just two months per year.\n",
    "# Counts from just a few hours of the day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f86accd",
   "metadata": {},
   "source": [
    "## Filter & Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6949801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to remove space in column names.\n",
    "inner_london.columns = inner_london.columns.str.replace(' ','_')\n",
    "central_london.columns = central_london.columns.str.replace(' ','_')\n",
    "outer_london.columns = outer_london.columns.str.replace(' ','_')\n",
    "sydney_counts.columns = sydney_counts.columns.str.replace(' ','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d998e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove French Day name in Survey Date. \n",
    "inner_london[\"Survey_date\"] = inner_london[\"Survey_date\"].str.replace(r'\\D+', '', regex=True)\n",
    "central_london[\"Survey_date\"] = central_london[\"Survey_date\"].str.replace(r'\\D+', '', regex=True)\n",
    "outer_london[\"Survey_date\"] = outer_london[\"Survey_date\"].str.replace(r'\\D+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e36c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop empty columns.\n",
    "ny_counts.dropna(how='all', axis=1, inplace=True)\n",
    "inner_london.dropna(how='all', axis=1, inplace=True)\n",
    "central_london.dropna(how='all', axis=1, inplace=True)\n",
    "outer_london.dropna(how='all', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fae914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse London dataframe Dates into appropriate format.\n",
    "\n",
    "# Convert to datetime format.\n",
    "inner_london['Survey_date'] = pd.to_datetime(inner_london['Survey_date'], dayfirst=True, yearfirst=False)\n",
    "central_london['Survey_date'] = pd.to_datetime(central_london['Survey_date'], dayfirst=True, yearfirst=False)\n",
    "outer_london['Survey_date'] = pd.to_datetime(outer_london['Survey_date'], dayfirst=True, yearfirst=False)\n",
    "\n",
    "# Sort dataframe in chronological order.\n",
    "inner_london = inner_london.sort_values(['Survey_date', 'Start_hour', 'Start_minute'])\n",
    "central_london = central_london.sort_values(['Survey_date', 'Start_hour', 'Start_minute'])\n",
    "outer_london = outer_london.sort_values(['Survey_date', 'Start_hour', 'Start_minute'])\n",
    "\n",
    "# Replace with value of previous value row where there is a missing value in Survey_date.\n",
    "inner_london['Survey_date'].fillna(method='ffill', inplace=True)\n",
    "central_london['Survey_date'].fillna(method='ffill', inplace=True)\n",
    "outer_london['Survey_date'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Define day of the week in English and add back as a column.\n",
    "inner_london['Day_of_week'] = inner_london['Survey_date'].dt.day_name()\n",
    "central_london['Day_of_week'] = central_london['Survey_date'].dt.day_name()\n",
    "outer_london['Day_of_week'] = outer_london['Survey_date'].dt.day_name()\n",
    "\n",
    "# Pass Month into a new column.\n",
    "inner_london['month'] = inner_london['Survey_date'].dt.month\n",
    "central_london['month'] = central_london['Survey_date'].dt.month\n",
    "outer_london['month'] = outer_london['Survey_date'].dt.month\n",
    "\n",
    "# Pass Year into a new column.\n",
    "inner_london['year'] = inner_london['Survey_date'].dt.year\n",
    "central_london['year'] = central_london['Survey_date'].dt.year\n",
    "outer_london['year'] = outer_london['Survey_date'].dt.year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab89ebcc",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "**Change the locale to 'en_GB' for every instance of 'English' if the local machine is Mac**\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0402c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse London dataframe Dates into appropriate format\n",
    "# Pass Month name into a new column.\n",
    "inner_london['month_name'] = inner_london['Survey_date'].dt.month_name(locale='English')\n",
    "central_london['month_name'] = central_london['Survey_date'].dt.month_name(locale='English')\n",
    "outer_london['month_name'] = outer_london['Survey_date'].dt.month_name(locale='English')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc61b3",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "**Change the locale to 'en_GB' for every instance of 'English' if the local machine is Mac**\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5483ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse NY dataframe Dates into appropriate formats.\n",
    "# Extract year information from timestamped date column.\n",
    "ny_counts['year'] = ny_counts['date'].dt.year\n",
    "\n",
    "# Extract hour information first from timestamped date column.\n",
    "ny_counts['hour'] = ny_counts['date'].dt.hour\n",
    "\n",
    "# Extract month name from the timestamped date column.\n",
    "ny_counts['month_name'] = ny_counts['date'].dt.month_name(locale='en_GB')\n",
    "\n",
    "# Extract month number from the timestamped date column.\n",
    "ny_counts['month'] = ny_counts['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7183ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass Season.\n",
    "# Will need this to test for seasonal impact on cycling uptake.\n",
    "# Create function for user defined seasons.\n",
    "def f(x):\n",
    "    if (x >= 1) and (x <= 2):\n",
    "        return 'Winter'\n",
    "    elif (x > 2) and (x <= 5 ):\n",
    "        return 'Spring'\n",
    "    elif (x > 5) and (x <= 8):\n",
    "        return'Summer'\n",
    "    elif (x > 8) and (x <= 11) :\n",
    "        return 'Autumn'\n",
    "    elif (x > 11):\n",
    "        return'Winter'\n",
    "\n",
    "# Apply user defined function to create new column with seasons.\n",
    "inner_london['season'] = inner_london['month'].apply(f)\n",
    "central_london['season'] = central_london['month'].apply(f)\n",
    "outer_london['season'] = outer_london['month'].apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ceca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup NY Dates\n",
    "# Ensure the data is sorted in chronological order without altering anything else.\n",
    "ny_counts.sort_values(by='date')\n",
    "\n",
    "# Data set runs from 31 Aug 2012 to 04 Jul 2022.\n",
    "# Extract hour of the day to identify patterns. Expect peak vs off peak patterns.\n",
    "# Group the data by day of the week to look for patterns around that.\n",
    "# Group the data into month to look for seasonal patterns.\n",
    "# Group/subset the data by id which is another spatial/location identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee88da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user defined function for peak and off peak hours to match London data.\n",
    "# See when and if time of the day matters. \n",
    "def f(x):\n",
    "    if (x > 6) and (x <= 7):\n",
    "        return 'Early Morning'\n",
    "    elif (x > 7) and (x <= 10 ):\n",
    "        return 'AM peak'\n",
    "    elif (x > 10) and (x <= 16):\n",
    "        return'Inter Peak'\n",
    "    elif (x > 16) and (x <= 19) :\n",
    "        return 'PM Peak'\n",
    "    elif (x > 19) and (x <= 23):\n",
    "        return'Evening'\n",
    "    elif (x <= 6):\n",
    "        return'Night'\n",
    "    \n",
    "# Apply user defined function to create new column with peak and off peak hours.\n",
    "ny_counts['time_of_day'] = ny_counts['hour'].apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define day of the week and add a column to match London Data.\n",
    "ny_counts = ny_counts.assign(day_of_week = lambda x: x.date.dt.day_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3ad2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define month to calculate user defined seasons to match London.\n",
    "# Extract month information first from timestamped date column.\n",
    "ny_counts['month'] = ny_counts['date'].dt.month\n",
    "\n",
    "# Create user defined function for seasons.\n",
    "def f(x):\n",
    "    if (x >= 1) and (x <= 2):\n",
    "        return 'Winter'\n",
    "    elif (x > 2) and (x <= 5 ):\n",
    "        return 'Spring'\n",
    "    elif (x > 5) and (x <= 8):\n",
    "        return'Summer'\n",
    "    elif (x > 8) and (x <= 11) :\n",
    "        return 'Autumn'\n",
    "    elif (x > 11):\n",
    "        return'Winter'\n",
    "\n",
    "# Apply user defined function to create new column with seasons.\n",
    "ny_counts['season'] = ny_counts['month'].apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c6fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the order of the columns so they flow more logically within the dataframe.\n",
    "neworder = ['id', 'date', 'year', 'month', 'month_name', 'season', \n",
    "            'day_of_week', 'hour', 'time_of_day', 'counts', 'status']\n",
    "ny_counts = ny_counts.reindex(columns=neworder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c1c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columns of NY_counts.\n",
    "print(ny_counts.status.unique())\n",
    "\n",
    "# As both status within acceptable boundaries versus data dictionary, can drop status column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b29a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant columns.\n",
    "ny_counts=ny_counts.drop(['status'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63bf8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename the Sydney columns to align to Peak/off Peak of other dataframes.\n",
    "sydney_counts.rename(columns = {\"Time_0600\":\"Early_Morning\",\n",
    "                                \"Time_0700\":\"AM_Peak1\",\n",
    "                                \"Time_0800\":\"AM_Peak2\",\n",
    "                                \"Time_1600\":\"PM_Peak1\",\n",
    "                                \"Time_1700\":\"PM_Peak2\",\n",
    "                                \"Time_1800\":\"PM_Peak3\"},\n",
    "                                inplace=True)\n",
    " \n",
    "# Add All AM Peak and PM Peak Columns into 2 columns to be consistent with other dataframes.\n",
    "sydney_counts['AM_Peak'] = sydney_counts['AM_Peak1'] + sydney_counts['AM_Peak2'] \n",
    "sydney_counts['PM_Peak'] = sydney_counts['PM_Peak1'] + sydney_counts['PM_Peak2']\\\n",
    "                         + sydney_counts['PM_Peak3']\n",
    "\n",
    "# View output with 5 random samples .\n",
    "sydney_counts.sample(5)\n",
    "\n",
    "# Can consolidate some columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17287313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where total cycles doesnt add up to number of private and hire cycles.\n",
    "# Create a column to sum the values.\n",
    "sydney_counts['Sum'] = sydney_counts['AM_Peak'] + sydney_counts['PM_Peak']\\\n",
    "                     + sydney_counts['Early_Morning']\n",
    "\n",
    "# Drop rows where the column values dont add up.\n",
    "sydney_counts = sydney_counts[sydney_counts.TotalCount == sydney_counts.Sum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c1fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant columns.\n",
    "sydney_counts=sydney_counts.drop(['ObjectId2', \n",
    "                                  'AM_Peak1', \n",
    "                                  'AM_Peak2', \n",
    "                                  'PM_Peak1', \n",
    "                                  'PM_Peak2', \n",
    "                                  'PM_Peak3', \n",
    "                                  'Sum'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754ce8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where total cycles doesnt add up to number of private and hire cycles.\n",
    "# Create a column to sum the values.\n",
    "inner_london['Sum'] = inner_london['Number_of_private_cycles']\\\n",
    "                    + inner_london['Number_of_cycle_hire_bikes']\n",
    "\n",
    "# Drop rows where the column values dont add up.\n",
    "inner_london = inner_london[inner_london.Total_cycles == inner_london.Sum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where total cycles doesnt add up to number of private and hire cycles.\n",
    "# Creating a column to add up the values.\n",
    "central_london['Sum'] = central_london['Number_of_private_cycles']\\\n",
    "                      + central_london['Number_of_cycle_hire_bikes']\n",
    "\n",
    "# Drop rows where the column values dont add up.\n",
    "central_london = central_london[central_london.Total_cycles == central_london.Sum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c5ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where total cycles doesnt add up to number of private and hire cycles.\n",
    "# Create a column to add up the values.\n",
    "outer_london['Sum'] = outer_london['Number_of_male_cycles']\\\n",
    "                    + outer_london['Number_of_female_cycles']\\\n",
    "                    + outer_london['Number_of_unknown_cycles']\n",
    "\n",
    "# Drop rows where the column values dont add up.\n",
    "outer_london = outer_london[outer_london.Total_cycles == outer_london.Sum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f7493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the decimal point in Survey_wave.\n",
    "inner_london['Survey_wave_(year)'] = inner_london['Survey_wave_(year)']\\\n",
    "                                    .astype(str).apply(lambda x: x.replace('.0','')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb60e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant columns.\n",
    "inner_london=inner_london.drop(['Sum', 'Start_hour', 'Start_minute'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce08a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the additional information in Survey wave column. \n",
    "# Now consistent with other London datasets.\n",
    "central_london[\"Survey_wave_(calendar_quarter)\"] = central_london[\"Survey_wave_(calendar_quarter)\"]\\\n",
    "                                                  .str.replace(r'\\D+', '', regex=True)\n",
    "\n",
    "# Drop the last number in every row.\n",
    "central_london['Survey_wave_(calendar_quarter)'] = central_london['Survey_wave_(calendar_quarter)']\\\n",
    "                                                  .astype(str).str[:-1].astype(np.int64)\n",
    "\n",
    "# Rename Column Name to align with other London Datasets\n",
    "central_london.rename(columns={'Survey_wave_(calendar_quarter)': 'Survey_wave_(year)'},\n",
    "                                inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58623e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant columns.\n",
    "central_london=central_london.drop(['Sum',\n",
    "                                    'Start_hour',\n",
    "                                    'Start_minute',\n",
    "                                    'Equivalent_financial_quarter'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff224022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant columns.\n",
    "outer_london=outer_london.drop(['Sum', \n",
    "                                'Start_hour',\n",
    "                                'Start_minute'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values in each column.\n",
    "for col in inner_london:\n",
    "  print(col,\": \", inner_london[col].nunique())\n",
    "\n",
    "# More site ids vs location.\n",
    "# Survey period of over 7 years.\n",
    "# 5 Periods of day which should be synced in same was as all the other city count data.\n",
    "# London period of day definition to be used as base.\n",
    "# 165 weather types need to be consolidated for better analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46795a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the different types of weather.\n",
    "print(inner_london.Weather.unique())\n",
    "\n",
    "# Lots of overlaps for e.g. Rain & Wet, Dry/cold and dry Cold.\n",
    "# Need to classify into much narrower streams. \n",
    "# Cannot meaningfully look for patterns amongst 165 weather conditions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb276734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate descriptions in weather.\n",
    "# Rain\n",
    "inner_london['Weather'] = inner_london['Weather'].replace(['Wet','Cloudy/rain','Rain','Mix Wet/dry','Drizzle',\n",
    "                                                          'Light Showers', 'Mizzle','Windy/rain','Showers',\n",
    "                                                          'Wet/dry','Wet/damp','Shower','Drizzle/shower','Rainy',\n",
    "                                                          'wet','Cloudy with showers','Generally overcast brief shower'\n",
    "                                                          'Light Rain','Shower/dry','Spitting','Drizzle/cloudy',\n",
    "                                                          'Dry/wet','Damp', 'Dry/drizzle','Dull/damp','Dry-wet',\n",
    "                                                          'Wet/mix', 'Drizzle/wet','Wet/windy','Rain Shower',\n",
    "                                                          'Intermittent Showers','Cloudy/drizzle','Rain/drizzle',\n",
    "                                                          'Wet Road','Drizzle/dry','Drizzle/rain','Mixed Sunny + Rain',\n",
    "                                                          'Wet/rain', 'V Light Drizzle', 'Rainy', 'W','Slight Drizzle',\n",
    "                                                          'Rain Stopped', 'Stopped Raining','Wet Rain Stopped','Raining/wet',\n",
    "                                                          'Showery','Overcast/rain','Rain/wet','Rain/showers','Showers/sunny',\n",
    "                                                          'Drizzle/showers','Wet/stop Raining','Drizzle Rain','Drizzle Wet',\n",
    "                                                          'Damp/sun','Raining','Dry + Wet','Showers/cloudy','Cloudy/showers',\n",
    "                                                          'Getting Wet','Wet Road:sun','Dry But Wet Road','Drizze',\n",
    "                                                          'wet','Wettish','Light Rain','S.wet','S/w',\n",
    "                                                          'Heavy Rain','Heavy Shower','Heavy Shr','Down Pour',\n",
    "                                                           'Deluge','Heavy Showers', 'Shower','Rain Heavy Showers',\n",
    "                                                           'Intermitent Showers','Thunder Lightening Rain!','Very Wet',\n",
    "                                                           'V.wet','Heavy Downpour/rain','Showery','Wet Heavy Rain',\n",
    "                                                           'Wet (heavy Rain)','Wet (shower)'],'Rain')\n",
    "\n",
    "# Good\n",
    "inner_london['Weather'] = inner_london['Weather'].replace(['Sunny','Cloudy Sunny','Sun Setting','Good','Dry/sunny',\n",
    "                                                          'Fine + Dry', 'Fine + Hot','Bright','Dry Hot!!',\n",
    "                                                          'Dry & Sunny','Dry & Sun','Fine & Dry','Good/dry','Sun',\n",
    "                                                          'Sunny Dry','Clear and Bright', 'Fine', 'Dry/good', \n",
    "                                                          'Fine/dry', 'Warm + Dry','Dry','Dry                         9',\n",
    "                                                          'Sunny','Cloudy/sunny','Druy','Dry/hot','Dry Warm',\n",
    "                                                          'Dry/sun','Dryish','Clear And Dry','Clear and Dry','Dry, Warm',\n",
    "                                                          'Dry, Sunny, Warm','Cloudy with Clear Intervals','Clear and Warm',\n",
    "                                                          'Dry But Misty','Sunny & Warm All Day','Clear','Dry + Sunny',\n",
    "                                                          'Sunny/dry','Dr Ry','Dry Y','D','Warm/dry','Bright/dry','Dry Sunny',\n",
    "                                                          'Fair','Dry/sun','Cloudy','Sunny Overcast Sunny','Sunny/cloudy','Cloudy/rain/sunny',\n",
    "                                                           'Cloudy + Sunny','Sunny + Cloudy', 'Cloudy/sunny',\n",
    "                                                           'Bright + Cloudy','Cloudy/dry','Partly Sunny','Dull','Dry & Mild',\n",
    "                                                           'Cloud','Overcast','Mild','Overcast (No Rain)',\n",
    "                                                          'Cloudy bright intervals','Generally overcast',\n",
    "                                                           'Cloudy with clear spells','Sunny Overcast','Dry',\n",
    "                                                           'Dry/mild', 'Clear','Cloudy and Dry','Partly cloudy but dry',\n",
    "                                                          'Partly cloudy and dry','Cloudy but dry','Partly cloudy and Dry',\n",
    "                                                          'Sun/Cloudy','Clouds & Sunny','Sun/clouds','Cloudy & Sunny',\n",
    "                                                          'Sun & Clouds','Cloudy Dry','Cloud/sun','Mixed','Sun/cloud',\n",
    "                                                           'Sunny/cloudy','Cloudy Sun','Cloudy/sun','Dry/cloudy',\n",
    "                                                           'Sun/cloudy','Overcast/dry','Cloud','Dull','Dry/overcast',\n",
    "                                                          'Dark/cloudy','Cloudy/dry','Cloudy'],'Good')\n",
    "\n",
    "\n",
    "\n",
    "# Damp\n",
    "inner_london['Weather'] = inner_london['Weather'].replace(['Wet/dry','Intermittent Light Drizzle','Light Rain',\n",
    "                                                           'Lt Rain','Drizzle','Intermittent Drizzle', 'Damp','Getting Dry',\n",
    "                                                           'Dry & Wet','Slight Drizzle/dry','Dry Road Still Wet'],'Damp')\n",
    "\n",
    "# Dangerous Conditions\n",
    "inner_london['Weather'] = inner_london['Weather'].replace(['Heavy Rain','Dry/wet Road','Dry With Wet Road',\n",
    "                                                           'Hot','Snow!','Snow', 'Sleet','Very Hot',\n",
    "                                                           'Dry (road Wet)','Dry, Sunny, Hot','Very Heavy Rain',\n",
    "                                                           'Intermittent Heavy Showers','Very Hot/dry','Hot/dry',\n",
    "                                                           'Storm','Heavy Rain High Winds','V Wet','Rain Heavy',\n",
    "                                                          'Sunny (hot!)','Heavy Thunder','Overcast/rain Heavy Showers',\n",
    "                                                          'Too Cold','High Wind','Very Windy','Wet/windy','Wet/v.windy',\n",
    "                                                           'Wet Hail','Rain/hail','Foggy Wet',\n",
    "                                                           'Wet Heavy Wind', 'Wet-windy','Hailstones',\n",
    "                                                           'Short Hail Shower','Rain/sleet','Hail Stone',\n",
    "                                                          'Hail','Showers/hailstone','Rain/hailstone','Dry Chill','Dry/cold',\n",
    "                                                           'Dry Cold','Cold/sunny','Cold/cloudy',\n",
    "                                                           'Dry Very Windy', 'Dry/windy','Windy','Cold','Cloudy/windy',\n",
    "                                                           'Windy + Sunny','Sunsetting + Windy','Dark Cloudy',\n",
    "                                                           'Dry V. Cold!','Very Cool','Dry & Windy',\n",
    "                                                          'Dry but Cold or Wind','Dry/v. Windy','Dry Windy',\n",
    "                                                          'Windy At First Then Sunny','Windy Dry',\n",
    "                                                          'Dry Wet Road','Thunder'],'Dangerous_Conditions')\n",
    "\n",
    "# Consolidating 'Unknown'\n",
    "inner_london['Weather'] = inner_london['Weather'].replace(['School Out','N/a','Unknown'],'Unknown')\n",
    "\n",
    "# Transforming Nan Values into Unknown.\n",
    "# Replacing nan with 'Unknown'.\n",
    "inner_london.Weather = inner_london.Weather.fillna('Unknown')\n",
    "\n",
    "# Consolidating \"Dry Dark\" into \"Unknown\".\n",
    "inner_london['Weather'] = inner_london['Weather'].replace(['Dry Dark','Dry/dark','Dark/dry',\n",
    "                                                           'Dark Dry'],'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f22f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values in each column.\n",
    "for col in central_london:\n",
    "  print(col,\": \", central_london[col].nunique())\n",
    "\n",
    "# More site ids vs location again.\n",
    "# Survey period of 8 years\n",
    "# 5 Periods of day which should be synched with all the other city count data.\n",
    "# Use london period of day definition as base.\n",
    "# 283 types of weather need to be consolidated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c664ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate descriptions in weather.\n",
    "# Rain\n",
    "central_london['Weather'] = central_london['Weather'].replace(['Wet','Cloudy/rain','Rain','Mix Wet/dry','Drizzle',\n",
    "                                                          'Light Showers', 'Mizzle','Windy/rain','Showers',\n",
    "                                                          'Wet/dry','Wet/damp','Shower','Drizzle/shower','Rainy',\n",
    "                                                          'wet','Cloudy with showers','Generally overcast brief shower'\n",
    "                                                          'Light Rain','Shower/dry','Spitting','Drizzle/cloudy',\n",
    "                                                          'Dry/wet','Damp', 'Dry/drizzle','Dull/damp','Dry-wet',\n",
    "                                                          'Wet/mix', 'Drizzle/wet','Wet/windy','Rain Shower',\n",
    "                                                          'Intermittent Showers','Cloudy/drizzle','Rain/drizzle',\n",
    "                                                          'Wet Road','Drizzle/dry','Drizzle/rain','Mixed Sunny + Rain',\n",
    "                                                          'Wet/rain', 'V Light Drizzle', 'Rainy', 'W','Slight Drizzle',\n",
    "                                                          'Rain Stopped', 'Stopped Raining','Wet Rain Stopped','Raining/wet',\n",
    "                                                          'Showery','Overcast/rain','Rain/wet','Rain/showers','Showers/sunny',\n",
    "                                                          'Drizzle/showers','Wet/stop Raining','Drizzle Rain','Drizzle Wet',\n",
    "                                                          'Damp/sun','Raining','Dry + Wet','Showers/cloudy','Cloudy/showers',\n",
    "                                                          'Getting Wet','Wet Road:sun','Dry But Wet Road','Drizze',\n",
    "                                                          'wet','Wettish','Light Rain','S.wet','S/w','Cold/rain',\n",
    "                                                           'Slightly Wet','Road Wet','Light Shower','Rain Damp','Wet Damp',\n",
    "                                                              'Wet - Dry','Dry - Wet','Rain Dry','Dry - Rain','Damp - Rain',\n",
    "                                                              'Wet/ Dry','S. Wet','Cloudy/ Rain','Windy/ Rain','Wet T',\n",
    "                                                              'Some Showers','Rains','Sunny/rainy','Wetr','Showers Mix',\n",
    "                                                              'Rain/dry','Rain/cloudy','Shower/wet','Wetter',\n",
    "                                                              'Heavy Rain','Heavy Shower','Heavy Shr','Down Pour',\n",
    "                                                           'Deluge','Heavy Showers', 'Shower','Rain Heavy Showers',\n",
    "                                                           'Intermitent Showers','Thunder Lightening Rain!','Very Wet',\n",
    "                                                           'V.wet','Heavy Downpour/rain','Showery','Wet Heavy Rain',\n",
    "                                                           'Wet (heavy Rain)','Wet (shower)','Blustery','V. Wet',\n",
    "                                                              'Rain & Thunder','Rain-heavy','H Rain','Wert','(rain After)',\n",
    "                                                              'Cloud/rain','Really Wet','Periods Of Rain Quite Windy',\n",
    "                                                              'Steady Rain'],'Rain')\n",
    "\n",
    "# Good\n",
    "central_london['Weather'] = central_london['Weather'].replace(['Sunny','Cloudy Sunny','Sun Setting','Good','Dry/sunny',\n",
    "                                                          'Fine + Dry', 'Fine + Hot','Bright','Dry Hot!!',\n",
    "                                                          'Dry & Sunny','Dry & Sun','Fine & Dry','Good/dry','Sun',\n",
    "                                                          'Sunny Dry','Clear and Bright', 'Fine', 'Dry/good', \n",
    "                                                          'Fine/dry', 'Warm + Dry','Dry','Dry                         9',\n",
    "                                                          'Sunny','Cloudy/sunny','Druy','Dry/hot','Dry Warm',\n",
    "                                                          'Dry/sun','Dryish','Clear And Dry','Clear and Dry','Dry, Warm',\n",
    "                                                          'Dry, Sunny, Warm','Cloudy with Clear Intervals','Clear and Warm',\n",
    "                                                          'Dry But Misty','Sunny & Warm All Day','Clear','Dry + Sunny',\n",
    "                                                          'Sunny/dry','Dr Ry','Dry Y','D','Warm/dry','Bright/dry','Dry Sunny',\n",
    "                                                          'Fair','Dry/sun','Kdry','Fine Windy',\n",
    "                                                               'Cloudy','Sunny Overcast Sunny','Sunny/cloudy',\n",
    "                                                               'Cloudy/rain/sunny','Cloudy + Sunny','Sunny + Cloudy',\n",
    "                                                               'Cloudy/sunny','Bright + Cloudy','Cloudy/dry',\n",
    "                                                               'Partly Sunny','Dull','Dry & Mild','Cloud','Overcast',\n",
    "                                                               'Mild','Overcast (No Rain)','Cloudy bright intervals',\n",
    "                                                               'Generally overcast','Cloudy with clear spells',\n",
    "                                                               'Sunny Overcast','Dry','Dry/mild', 'Clear',\n",
    "                                                               'Cloudy and Dry','Partly cloudy but dry',\n",
    "                                                          'Partly cloudy and dry','Cloudy but dry','Partly cloudy and Dry',\n",
    "                                                          'Sun/Cloudy','Clouds & Sunny','Sun/clouds','Cloudy & Sunny',\n",
    "                                                          'Sun & Clouds','Cloudy Dry','Cloud/sun','Mixed','Sun/cloud',\n",
    "                                                           'Sunny/cloudy','Cloudy Sun','Cloudy/sun','Dry/cloudy',\n",
    "                                                           'Sun/cloudy','Overcast/dry','Cloud','Dull','Dry/overcast',\n",
    "                                                          'Dark/cloudy','Cloudy/dry','Cloudy','Hazy','Partly Cloudy',\n",
    "                                                               'Drty','Dry (windy)','Fine (windy)','Sunny Cloudy',\n",
    "                                                              'Dry Dark','Dark','Dry Mon','Dry Wed','Dry Thu','Dry Fri',\n",
    "                                                              'Sun/rain','Thunder','Cloudy','Sunny Overcast Sunny',\n",
    "                                                               'Sunny/cloudy','Cloudy/rain/sunny',\n",
    "                                                           'Cloudy + Sunny','Sunny + Cloudy', 'Cloudy/sunny',\n",
    "                                                           'Bright + Cloudy','Cloudy/dry','Partly Sunny','Dull','Dry & Mild',\n",
    "                                                           'Cloud','Overcast','Mild','Overcast (No Rain)',\n",
    "                                                          'Cloudy bright intervals','Generally overcast',\n",
    "                                                           'Cloudy with clear spells','Sunny Overcast','Dry',\n",
    "                                                           'Dry/mild', 'Clear','Cloudy and Dry','Partly cloudy but dry',\n",
    "                                                          'Partly cloudy and dry','Cloudy but dry','Partly cloudy and Dry',\n",
    "                                                          'Sun/Cloudy','Clouds & Sunny','Sun/clouds','Cloudy & Sunny',\n",
    "                                                          'Sun & Clouds','Cloudy Dry','Cloud/sun','Mixed','Sun/cloud',\n",
    "                                                           'Sunny/cloudy','Cloudy Sun','Cloudy/sun','Dry/cloudy',\n",
    "                                                           'Sun/cloudy','Overcast/dry','Cloud','Dull','Dry/overcast',\n",
    "                                                          'Dark/cloudy','Cloudy/dry','Cloudy','Hazy','Partly Cloudy',\n",
    "                                                               'Drty','Dry (windy)','Fine (windy)','Sunny Cloudy',\n",
    "                                                              'Dry Dark','Dark','Dry Mon','Dry Wed','Dry Thu','Dry Fri',\n",
    "                                                              'Sun/rain','Thunder','Ddry','Dy','Dry/sunny/cold','Fine Cold',\n",
    "                                                              'Cold Dry','Dry & Cold','Dry And Fine','Dry And Sunny',\n",
    "                                                              'Dry And Warm','Fine And Dry','Warm + Sunny','Warm And Humid',\n",
    "                                                              'Warm And Windy','Overcast And Dull','Cloudy And Warm',\n",
    "                                                              'Sunny Periods And Warm','Dry And Windy','Dry And Very Windy',\n",
    "                                                              'Warm Sunny And Windy','Hot And Humid','Mild And Sunny',\n",
    "                                                               'Warm And Overcast','Sunny & Windy','Windy/cloudy',\n",
    "                                                              'Dry/gusty','Coldish','Windy/dry','Dry But A Bit Windy',\n",
    "                                                               'Sunny Cold','Cold At First Then Warm/sunny',\n",
    "                                                              'Warm & Sunny Chilly Later','Fine + Dry Chilly At First',\n",
    "                                                               'Fine & Sunny','dry','A Bit Chilly At First',\n",
    "                                                               'Warm With A Slight Wind','Cold Then Dry And Windy',\n",
    "                                                               'Dry And Overcast','Warm + Sunny Cloudy + Windy',\n",
    "                                                              'Dry 3/4 Dry','Sunny Until Evening But Windy',\n",
    "                                                               'Winds Rather Chilly','Warm','Sunny But Very Windy',\n",
    "                                                               'Now Starts To Get Chilly'],'Good')\n",
    "\n",
    "\n",
    "# Light Rain\n",
    "central_london['Weather'] = central_london['Weather'].replace(['Wet/dry','Intermittent Light Drizzle','Light Rain',\n",
    "                                                           'Lt Rain','Drizzle','Intermittent Drizzle', 'Damp','Getting Dry',\n",
    "                                                           'Dry & Wet','Slight Drizzle/dry','Wet Intermittently',\n",
    "                                                               'Light Rain','V Light Rain','Dry Wet Road','Dry A.m Wet P.m',\n",
    "                                                               'Mist','Road Drying Sun Out','Wetish','Light Shrs',\n",
    "                                                              'Fine Drizzle','V Light Shrs','L/rain','Rain Stopped-dry',\n",
    "                                                              'V Lt Rain','V.light Rain','Dry (+brief Speels Of Drizzle',\n",
    "                                                              'Wet (spitting)','Drizzly Rain','Almost Dry','Damp & Drizzly',\n",
    "                                                              'Dry Road Wet With Leaves','Wet Drizzle','No Rain Wet Roads',\n",
    "                                                              'Dry But Wet Roads','Very Light Rain','Light Drizzle',\n",
    "                                                              'Dry/wet Road Surface','V Light Showers','V. Light Rain',\n",
    "                                                              'Wet/cloudy','Wet/sunny','Dry Road Still Wet',\n",
    "                                                              '2 Snowflakes Otherwise Dry','Wet-dry','Dry/drizzly',\n",
    "                                                              'Wet/light Showers','Wet/drizzle','Wet And Windy',\n",
    "                                                              'Drizzling','Drizzle Damp','Windy Showery','Wet + Dry',\n",
    "                                                              'V.light Drizzle','Very Light Drizzle','Drying Up','Wet Again',\n",
    "                                                              'Cold Sunny Rain','Wet First Then Dry','Wetr First Then Dry',\n",
    "                                                              'Dry With Intermitent Rain','(drizzle)','Damp/misty/wet',\n",
    "                                                              'Dry But Rain Threatening','Slight Drizzle Till End',\n",
    "                                                              'Damp/misty','Cold & Dry Early Rain Later',\n",
    "                                                              'Wet ','Windy/drizzle','Intermitent Light Showers',\n",
    "                                                              'Intermitent Light Rain','A Few Rain Showers','Drizzly',\n",
    "                                                              'Rain Looking Likely','A Few Drops Of Rain'],'Damp')\n",
    "\n",
    "# Dangerous Weather\n",
    "central_london['Weather'] = central_london['Weather'].replace(['Heavy Rain','Dry/wet Road','Dry With Wet Road',\n",
    "                                                           'Hot','Snow!','Snow', 'Sleet','Very Hot',\n",
    "                                                           'Dry (road Wet)','Dry, Sunny, Hot','Very Heavy Rain',\n",
    "                                                           'Intermittent Heavy Showers','Very Hot/dry','Hot/dry',\n",
    "                                                           'Storm','Heavy Rain High Winds','V Wet','Rain Heavy',\n",
    "                                                          'Sunny (hot!)','Heavy Thunder','Overcast/rain Heavy Showers',\n",
    "                                                          'Too Cold','High Wind','Very Windy','Dry & Very Windy',\n",
    "                                                              'Very Hot Dry','Wet/windy','Wet/v.windy','Wet Hail',\n",
    "                                                               'Rain/hail','Foggy Wet',\n",
    "                                                           'Wet Heavy Wind', 'Wet-windy','Hailstones',\n",
    "                                                           'Short Hail Shower','Rain/sleet','Hail Stone',\n",
    "                                                          'Hail','Showers/hailstone','Rain/hailstone','Cold/ Rain',\n",
    "                                                              'Foggy','Wet & Windy','Wet + Windy','Rain/wind',\n",
    "                                                              'Wet (windy)','Occasional Lt Snow Shrs',\n",
    "                                                              'Wet And Very Windy','Dry Chill','Dry/cold','Dry Cold',\n",
    "                                                               'Cold/sunny','Cold/cloudy',\n",
    "                                                           'Dry Very Windy', 'Dry/windy','Windy','Cold','Cloudy/windy',\n",
    "                                                           'Windy + Sunny','Sunsetting + Windy','Dark Cloudy',\n",
    "                                                           'Dry V. Cold!','Very Cool','Dry & Windy',\n",
    "                                                          'Dry but Cold or Wind','Dry/v. Windy','Dry Windy',\n",
    "                                                          'Windy At First Then Sunny','Windy Dry','Cold Windy Dry',\n",
    "                                                              'Cold/dry','Some Heavy Showers','Very Cold/dry',\n",
    "                                                              'Foggy/v Cold','Hail Shower','Snowing','Wet/ Snowing',\n",
    "                                                              'Heavy Snow','Dry/very Windy','Very Windy & Cold',\n",
    "                                                              'Wet Light Hailstone','Heavy Showers Throughout Day',\n",
    "                                                              'High Winds & Spits Of Rain','Fine V Cold',\n",
    "                                                              'Dry (frost & Fog)','V Cold Showers','Cold/showery',\n",
    "                                                              'Light Showers Inc Some Hail','Cloudy/hail','Cold Wind',\n",
    "                                                              'Hot & Sunny','Hot And Sunny','Dry/windy/strong Wind',\n",
    "                                                              'Hot + Humid','Very Cold Sunny But Windy'],'Dangerous_Conditions')\n",
    "\n",
    "# Consolidating 'Unknown'\n",
    "central_london['Weather'] = central_london['Weather'].replace(['School Out','N/a','Unknown','Dark Sunny',\n",
    "                                                              'Wed','Warm & Sunny But Windy & Cold'],'Unknown')\n",
    "\n",
    "# Transforming Nan Values into Unknown\n",
    "# Replacing nan with 'Unknown'\n",
    "central_london.Weather = central_london.Weather.fillna('Unknown')\n",
    "\n",
    "# Consolidating \"Dry Dark\"\n",
    "central_london['Weather'] = central_london['Weather'].replace(['Dry Dark','Dry/dark','Dark/dry',\n",
    "                                                           'Dark Dry', 'X'],'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f7af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values in each column.\n",
    "for col in outer_london:\n",
    "  print(col,\": \", outer_london[col].nunique())\n",
    "\n",
    "# More site ids vs location.\n",
    "# May imply multiple sites in same location. \n",
    "# Survey period of over 7 years.\n",
    "# 5 Periods of day which should be synched in same fashion with all the other city count data.\n",
    "# Use london period of day definition as base.\n",
    "# 124 types of weather needs to be consolidated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1a61ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate descriptions in weather.\n",
    "\n",
    "# Rain.\n",
    "outer_london['Weather'] = outer_london['Weather'].replace(['Wet','Showers','Rain','Cloudy + Rain','Rain & Cloudy',\n",
    "                                                          'Raining', 'Rain/cloudy','Wet/thunder','Light Showers',\n",
    "                                                          'Rain/showers','W','Wey','Drizzle/shower','Rainy',\n",
    "                                                          'wet','Cloudy with showers','Generally overcast brief shower',\n",
    "                                                          'Heavy Rain','Heavy Shower','Heavy Shr','Down Pour',\n",
    "                                                           'Deluge','Heavy Showers', 'Shower','Rain Heavy Showers',\n",
    "                                                           'Intermitent Showers','Thunder Lightening Rain!','Very Wet',\n",
    "                                                           'V.wet','Heavy Downpour/rain','Showery','Wet Heavy Rain',\n",
    "                                                           'Wet (heavy Rain)'],'Rain')\n",
    "\n",
    "# Good.\n",
    "outer_london['Weather'] = outer_london['Weather'].replace(['Cloudy','Sunny Overcast Sunny','Sunny/cloudy','Cloudy/rain/sunny',\n",
    "                                                           'Cloudy + Sunny','Sunny + Cloudy', 'Cloudy/sunny',\n",
    "                                                           'Bright + Cloudy','Cloudy/dry','Partly Sunny','Dull','Dry & Mild',\n",
    "                                                           'Cloud','Overcast','Mild','Overcast (No Rain)',\n",
    "                                                          'Cloudy bright intervals','Generally overcast',\n",
    "                                                           'Cloudy with clear spells','Sunny Overcast','Dry',\n",
    "                                                           'Dry/mild', 'Clear','Cloudy with clear spells',\n",
    "                                                          'Sunny Overcast', 'Sunny','Cloudy Sunny','Sun Setting','Good',\n",
    "                                                           'Dry/sunny','Fine + Dry', 'Fine + Hot','Bright','Dry Hot!!',\n",
    "                                                          'Dry & Sunny','Dry & Sun','Fine & Dry','Good/dry','Sun',\n",
    "                                                          'Sunny Dry','Clear and Bright', 'Fine', 'Dry/good', \n",
    "                                                          'Fine/dry','Warm + Dry','D'],'Good')\n",
    "# Damp.\n",
    "outer_london['Weather'] = outer_london['Weather'].replace(['Wet/dry','Intermittent Light Drizzle',\n",
    "                                                           'Light Rain','Lt Rain','Drizzle','Intermittent Drizzle', 'Damp',\n",
    "                                                           'Getting Dry','Dry & Wet','Dry/wet',],'Damp')\n",
    "\n",
    "\n",
    "# Dangerous conditions.\n",
    "outer_london['Weather'] = outer_london['Weather'].replace(['Dry Chill','Dry/cold','Dry Cold','Cold/sunny','Cold/cloudy',\n",
    "                                                           'Dry Very Windy', 'Dry/windy','Windy','Cold','Cloudy/windy',\n",
    "                                                           'Windy + Sunny','Sunsetting + Windy','Dark Cloudy',\n",
    "                                                           'Dry V. Cold!','Very Cool','Wet/windy','Wet/v.windy','Wet Hail',\n",
    "                                                           'Rain/hail','Foggy Wet','Wet Heavy Wind', 'Wet-windy','Hailstones',\n",
    "                                                           'Short Hail Shower','Heavy Rain','Dry/wet Road','Dry With Wet Road',\n",
    "                                                           'Hot','Snow!','Snow', 'Sleet','Very Hot','Dry (road Wet)'],\n",
    "                                                          'Dangerous_Conditions')\n",
    "\n",
    "# Replacing nan with 'Unknown'.\n",
    "outer_london.Weather = outer_london.Weather.fillna('Unknown')\n",
    "\n",
    "# Consolidating 'Unknown'.\n",
    "outer_london['Weather'] = outer_london['Weather'].replace(['Dry Dark','Dry/dark','Dark/dry','Dark Dry', 'N/a'],'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b80b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated values in the dataframes.\n",
    "# Duplicates are rows where values across all columns match with another row in the dataframe.\n",
    "duplicateRowsDF_inner_london = inner_london[inner_london.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View output.\n",
    "print(duplicateRowsDF_inner_london)\n",
    "\n",
    "# 17,862 rows of duplicates.\n",
    "# Will drop once all data cleaning is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b0a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated values in the dataframes.\n",
    "duplicateRowsDF_central_london = central_london[central_london.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace97daf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View output.\n",
    "print(duplicateRowsDF_central_london)\n",
    "\n",
    "# 431 rows of duplicates.\n",
    "# Will drop these but once all data cleaning is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated values in the dataframes.\n",
    "# Duplicates are rows where values across all columns match with another row in the dataframe.\n",
    "duplicateRowsDF_outer_london = outer_london[outer_london.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7abc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View output.\n",
    "print(duplicateRowsDF_outer_london)\n",
    "\n",
    "# No duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf5870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated rows across all column values\n",
    "duplicateRowsDF_ny = ny_counts[ny_counts.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2241b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See output of duplication check.\n",
    "print(duplicateRowsDF_ny)\n",
    "\n",
    "# No duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated rows across all column values.\n",
    "duplicateRowsDF_sydney = sydney_counts[sydney_counts.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e453ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See output of duplication check.\n",
    "print(duplicateRowsDF_sydney)\n",
    "\n",
    "# No duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aa91ca",
   "metadata": {},
   "source": [
    "### Summarising Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd41e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values.\n",
    "ny_counts.isnull().sum()\n",
    "\n",
    "# No Null Values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc69c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values.\n",
    "sydney_counts.isnull().sum()\n",
    "\n",
    "# No Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8006b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values.\n",
    "inner_london.isnull().sum()\n",
    "\n",
    "# Just 6 null values in Time. \n",
    "# Given the overall size of the dataframe, this is not worthwhile trying to investigate.\n",
    "# Will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9544139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values.\n",
    "central_london.isnull().sum()\n",
    "\n",
    "# No Null Values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675df2c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check missing values.\n",
    "outer_london.isnull().sum()\n",
    "\n",
    "# No Null Values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd588e",
   "metadata": {},
   "source": [
    "## Exploring Distribution of count data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f9a441",
   "metadata": {},
   "source": [
    "### New York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group NY Counts Data by year, month, day_of week & time of day.\n",
    "df = ny_counts.groupby(['year'], as_index=False)['counts'].count()\n",
    "df1= ny_counts.groupby(['month_name', 'month'], as_index=False)['counts'].count()\n",
    "df2 = ny_counts.groupby(['day_of_week'], as_index=False)['counts'].count()\n",
    "df3 = ny_counts.groupby(['time_of_day', 'hour'], as_index=False)['counts'].count()\n",
    "df4 = ny_counts.groupby(['year', 'month'], as_index=False)['counts'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44142d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values.\n",
    "df1.sort_values('month', inplace=True)\n",
    "df3.sort_values('hour', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dbac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise NY Counts data distribution by year.\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# Initialise matplotlib fiture.\n",
    "f, g = plt.subplots(figsize = (10,5))\n",
    "\n",
    "g=sns.barplot(x = df['year'], y = df['counts'])\n",
    "\n",
    "# Add legend and add title etc.\n",
    "g.set(ylabel = \"Number of counts\",\n",
    "     xlabel = \"Year\")\n",
    "g.set_title('Distribution of count data by year in New York')\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "# Show the plot.\n",
    "plt.show()\n",
    "\n",
    "# We dont have equal amounts of count data across the years.\n",
    "# Suggests lack of uniformity across the frequency distribution of the count data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949ee1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise NY Counts count data distribution by month.\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# Initialise matplotlib fiture.\n",
    "f, g = plt.subplots(figsize = (10,5))\n",
    "\n",
    "g=sns.barplot(x = df1['month_name'], y = df1['counts'])\n",
    "\n",
    "# Add legend and add title etc.\n",
    "g.set(ylabel = \"Number of counts\",\n",
    "     xlabel = \"Month\")\n",
    "g.set_title('Distribution of count data by month in New York')\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "# Show the plot.\n",
    "plt.show()\n",
    "\n",
    "# Uneven distribution confirmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3866eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise NY Counts data distribution by day of the week.\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# Initialise matplotlib fiture.\n",
    "f, g = plt.subplots(figsize = (10,5))\n",
    "\n",
    "g=sns.barplot(x = df2['day_of_week'], y = df2['counts'])\n",
    "\n",
    "# Add legend and add title etc.\n",
    "g.set(ylabel = \"Number of counts\",\n",
    "     xlabel = \"Day of the week\")\n",
    "g.set_title('Distribution of count data by day of the week in New York')\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "# Show the plot.\n",
    "plt.show()\n",
    "\n",
    "# Equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Ny Counts by time of day.\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# Initialise matplotlib figure.\n",
    "f, g = plt.subplots(figsize = (10,5))\n",
    "\n",
    "g=sns.barplot(x = df3['time_of_day'], y = df3['counts'])\n",
    "\n",
    "# Add legend and add title etc.\n",
    "g.set(ylabel = \"Number of counts\",\n",
    "     xlabel = \"Time of the day\")\n",
    "g.set_title('Distribution of count data by time of the day in New York')\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "# Show the plot.\n",
    "plt.show()\n",
    "\n",
    "# Equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b97391",
   "metadata": {},
   "source": [
    "### London"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f60f40b",
   "metadata": {},
   "source": [
    "Inner London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a0fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data.\n",
    "df = inner_london.groupby(['Survey_date'], as_index=False)['Total_cycles'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bdbaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a count rank column.\n",
    "# Define a private function to fill the count rank column with relative numbers.\n",
    "# If Total cycles is > 0 then fill with 1 to 10 in the value column.\n",
    "# Else 0.\n",
    "\n",
    "# function for assigning value to Total cycle count of the day for sorting.\n",
    "def f(x):\n",
    "    if (x < 1):\n",
    "        return '0'\n",
    "    if (x >= 1) and (x <= 100):\n",
    "        return '1'\n",
    "    elif (x > 100) and (x <= 300 ):\n",
    "        return '2.5'\n",
    "    elif (x > 300) and (x <= 600):\n",
    "        return'4.5'\n",
    "    elif (x > 600) and (x <= 1000) :\n",
    "        return '8'\n",
    "    elif (x > 1000):\n",
    "        return'10'\n",
    "     \n",
    "\n",
    "# Apply user defined function to create new column with seasons.\n",
    "df['count_rank'] = df['Total_cycles'].apply(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460fbd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Count Rank is a numeric value\n",
    "df['count_rank'] = pd.to_numeric(df['count_rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ca7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a Calendar Heatmap to display relative number of counts for each day in a calendar view\n",
    "# Create the plot\n",
    "fig = calplot(df, \n",
    "              x=\"Survey_date\", \n",
    "              y=\"count_rank\", \n",
    "              dark_theme=True, \n",
    "              gap=0,\n",
    "              years_title=True)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Darker the block, the more data there is for that day.\n",
    "# Lighter the block means less data for that day.\n",
    "# Black area means no data for that day.\n",
    "# Shows Data Distribution for inner London is very patchy.\n",
    "# Concentrated just between April and July most years.\n",
    "# Not evenly distributed even in that limited period.\n",
    "# No weekend counts.\n",
    "# No count in the usual summer months in 2020.\n",
    "# Instead counts in autumn months in 2020.\n",
    "# Normal counting activity returns in 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e733615",
   "metadata": {},
   "source": [
    "Outer London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b0fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data.\n",
    "df = outer_london.groupby(['Survey_date'], \n",
    "                          as_index=False)['Total_cycles'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cab079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a count rank column.\n",
    "# Define a private function to fill the count rank column with relative numbers.\n",
    "# If Total cycles is > 0 then fill with 1 to 10 in the value column.\n",
    "# Else 0.\n",
    "\n",
    "# Function to assig a value to Total cycle count of the day for sorting.\n",
    "def f(x):\n",
    "    if (x < 1):\n",
    "        return '0'\n",
    "    if (x >= 1) and (x <= 100):\n",
    "        return '1'\n",
    "    elif (x > 100) and (x <= 300 ):\n",
    "        return '2.5'\n",
    "    elif (x > 300) and (x <= 600):\n",
    "        return'4.5'\n",
    "    elif (x > 600) and (x <= 1000) :\n",
    "        return '8'\n",
    "    elif (x > 1000):\n",
    "        return'10'\n",
    "     \n",
    "\n",
    "# Apply user defined function to create new column with seasons.\n",
    "df['count_rank'] = df['Total_cycles'].apply(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aa8365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Count Rank is a numeric value.\n",
    "df['count_rank'] = pd.to_numeric(df['count_rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95efc2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a Calendar Heatmap to display relative number of counts for each day in a calendar view.\n",
    "# create the plot; same logic as Inner London\n",
    "fig = calplot(df, \n",
    "              x=\"Survey_date\", \n",
    "              y=\"count_rank\", \n",
    "              dark_theme=True, \n",
    "              gap=0, \n",
    "              colorscale = \"purples\", \n",
    "              years_title=True)\n",
    "fig.show()\n",
    "\n",
    "# Very similar patterns to Inner London.\n",
    "# Data Very patchy and not collected continously through the year.\n",
    "# Black areas are where there is 0 count.\n",
    "# As counts increase on a given day, blocks get darker purple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd9daf",
   "metadata": {},
   "source": [
    "Central London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b79a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "central_london.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac467fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data.\n",
    "df = central_london.groupby(['year'], as_index=False)['Total_cycles'].count()\n",
    "df1= central_london.groupby(['month_name', 'month'], as_index=False)['Total_cycles'].count()\n",
    "df2 = central_london.groupby(['Day_of_week'], as_index=False)['Total_cycles'].count()\n",
    "df3 = central_london.groupby(['Period', 'Time'], as_index=False)['Total_cycles'].count()\n",
    "df4 = central_london.groupby(['Survey_date'], as_index=False)['Total_cycles'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabcd945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values to improve chart presentation.\n",
    "df1.sort_values('month', inplace=True)\n",
    "df3.sort_values('Time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0bdb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Central London counts data distribution by year.\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# Initialise matplotlib figure.\n",
    "f, g = plt.subplots(figsize = (10,5))\n",
    "\n",
    "g=sns.barplot(x = df['year'], y = df['Total_cycles'])\n",
    "\n",
    "# Add legend and add title etc.\n",
    "g.set(ylabel = \"Number of counts\",\n",
    "      xlabel = \"Year\")\n",
    "\n",
    "g.set_title('Distribution of count data by year in Central London')\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "# Show the plot.\n",
    "plt.show()\n",
    "\n",
    "# Reasonably equal distribution.\n",
    "# Except 2020 which again matches Inner London where there is a dip in number of counts too.\n",
    "# Therefore during Covid this count service was impacted.\n",
    "# Data Collection should be automated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89da697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualise Central London count data distribution by month.\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# Initialise matplotlib figure.\n",
    "f, g = plt.subplots(figsize = (12,5))\n",
    "\n",
    "g=sns.barplot(x = df1['month_name'], y = df1['Total_cycles'])\n",
    "\n",
    "# Add legend and add title etc.\n",
    "g.set(ylabel = \"Number of counts\",\n",
    "     xlabel = \"Month\")\n",
    "g.set_title('Distribution of count data by month in Central London')\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "# Show the plot.\n",
    "plt.show()\n",
    "\n",
    "# Very Unequal distribution.\n",
    "# Data missing without any clear pattern.\n",
    "# But data is there across all months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab5a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Central London counts data distribution by day of the week.\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# Initialise matplotlib figure.\n",
    "f, g = plt.subplots(figsize = (10,5))\n",
    "\n",
    "g=sns.barplot(x = df2['Day_of_week'], y = df2['Total_cycles'])\n",
    "\n",
    "# Add legend and add title etc.\n",
    "g.set(ylabel = \"Number of counts\",\n",
    "      xlabel = \"Day of the week\")\n",
    "g.set_title('Distribution of count data by day of the week in Central London')\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "# Show the plot.\n",
    "plt.show()\n",
    "\n",
    "# Very uneven distribution although counts present every day.\n",
    "# Weekends clearly very uneven.\n",
    "# Given they are on the chart implies there is some data for weekends.\n",
    "# But too few to appear reasonably within the scale of the other data.\n",
    "# Again similar distribution to Inner & Outer London here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b75e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Central London by time of day.\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# Initialise matplotlib figure.\n",
    "f, g = plt.subplots(figsize = (12,5))\n",
    "\n",
    "g=sns.barplot(x = df3['Period'], y = df3['Total_cycles'])\n",
    "\n",
    "# Add legend and add title etc.\n",
    "g.set(ylabel = \"Number of counts\",\n",
    "     xlabel = \"Time of the day\")\n",
    "g.set_title('Distribution of count data by time of the day in Central London')\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "# Show the plot.\n",
    "plt.show()\n",
    "\n",
    "# Equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c3fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a count rank column.\n",
    "# Define a private function to fill the count rank column with relative numbers.\n",
    "# If Total cycles is > 0 then fill with 1 to 10 in the value column.\n",
    "# Else 0.\n",
    "\n",
    "# function for assigning value to Total cycle count of the day for sorting.\n",
    "def f(x):\n",
    "    if (x < 1):\n",
    "        return '0'\n",
    "    if (x >= 1) and (x <= 300):\n",
    "        return '1'\n",
    "    elif (x > 300) and (x <= 600 ):\n",
    "        return '2'\n",
    "    elif (x > 600) and (x <= 800):\n",
    "        return'3'\n",
    "    elif (x > 800) and (x <= 900) :\n",
    "        return '4'\n",
    "    elif (x > 900) and (x <= 1000):\n",
    "        return'5'\n",
    "    elif (x > 1000) and (x <= 1200) :\n",
    "        return '6'\n",
    "    elif (x > 1200) and (x <= 1500):\n",
    "        return'7'\n",
    "    elif (x > 1500) and (x <= 2000) :\n",
    "        return '8'\n",
    "    elif (x > 2000) and (x <= 3000):\n",
    "        return'9'\n",
    "    elif (x > 3000):\n",
    "        return'10'\n",
    "     \n",
    "\n",
    "# Apply user defined function to create new column with seasons.\n",
    "df4['count_rank'] = df4['Total_cycles'].apply(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Count Rank is a numeric value.\n",
    "df4['count_rank'] = pd.to_numeric(df4['count_rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c21649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a Calendar Heatmap to display relative number of counts for each day in a calendar view.\n",
    "# Create the plot; same logic as previous calendar heatmaps.\n",
    "fig = calplot(df4, x=\"Survey_date\", y=\"count_rank\", dark_theme=True, gap=0, \n",
    "              colorscale = \"blues\", years_title=True)\n",
    "fig.show()\n",
    "\n",
    "# Black areas which show no count data registered at all for those days.\n",
    "# Rest of the count data very patchy and incomplete too as displayed by the relative heatmap.\n",
    "# Counting stopped altogether in the 3 phases of lockdowns during Covid.\n",
    "# No weekend data similar to the other London data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b79a1",
   "metadata": {},
   "source": [
    "Sydney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f75246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data.\n",
    "df = sydney_counts.groupby(['Year'], as_index=False)['TotalCount'].count()\n",
    "df1= sydney_counts.groupby(['Month'], as_index=False)['TotalCount'].count()\n",
    "df2 = sydney_counts.groupby(['SiteID'], as_index=False)['TotalCount'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0581d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values to improve chart presentation.\n",
    "df.sort_values('Year', inplace=True)\n",
    "df1.sort_values('Month', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef1a2f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualise Sydney counts data distribution by year.\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# Initialise matplotlib figure.\n",
    "f, g = plt.subplots(figsize = (10,5))\n",
    "\n",
    "g=sns.barplot(x = df['Year'], y = df['TotalCount'])\n",
    "\n",
    "# Add legend and add title etc.\n",
    "g.set(ylabel = \"Number of counts\",\n",
    "     xlabel = \"Year\")\n",
    "g.set_title('Distribution of count data by year in Sydney')\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "# Show the plot.\n",
    "plt.show()\n",
    "\n",
    "# Sydney very consistent until 2017.\n",
    "# Sudden drop in count of counts after that.\n",
    "# Pick up of count data in Sydnet again 2019 \n",
    "# Again pretty uniform albeit consistenly fewer counts compared to pre-2018.\n",
    "# Was there a policy shift in 2018 w.r.t. cycling & thus resources pointed towards counting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27662e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Sydney count data distribution by month.\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# Initialise matplotlib figure\n",
    "f, g = plt.subplots(figsize = (10,5))\n",
    "\n",
    "g=sns.barplot(x = df1['Month'], y = df1['TotalCount'])\n",
    "\n",
    "# Add legend and add title etc.\n",
    "g.set(ylabel = \"Number of counts\",\n",
    "     xlabel = \"Month\")\n",
    "g.set_title('Distribution of count data by month in Sydney')\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "# Show the plot.\n",
    "plt.show()\n",
    "\n",
    "# Equally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87883d92",
   "metadata": {},
   "source": [
    "## Data Imputation or Deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f619224",
   "metadata": {},
   "source": [
    "New York & London have lots of patchy missingness.\n",
    "Further gap is missing weekend data in London.\n",
    "Strip New York of all weekend data which will then allow direct comparison between the cities.\n",
    "\n",
    "Missing data on weekends or some months only means MAR\n",
    "Some of the unevenness of data is also MCAR such as unevenesss between months in New York\n",
    "Most of the missing data in London is MAR\n",
    " \n",
    "New York data missingness is simplest as the data appears to be missing completely at random (MCAR)\n",
    "There is no observable link between missing data and the independent variables\n",
    "In order to test this statistically the categorical data needs to be converted \n",
    "Following the lo-fi presentation to ThoughtWorks, repairing data missingness was dropped\n",
    "It was deemed to be not worth the effort on balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74be18a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows in New York Count where day of the week is saturday or sunday.\n",
    "# This will align to fact London has no weekend data.\n",
    "# Generates a more like for like comparison between New York and London.\n",
    "# Drop rows containing Saturday or Sunday in day_of_week.\n",
    "ny_countsA = ny_counts[ny_counts[\"day_of_week\"].str.contains(\"Saturday|Sunday\") == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa3907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrames to csv.\n",
    "# Save the cleaned dataframes in NY and Sydney.\n",
    "# Additionally save the NY dataframe without the weekend day seperately.\n",
    "ny_counts.to_csv('ny_counts_cleaned_201022.csv')\n",
    "ny_countsA.to_csv('ny_counts_weekdays_only_201022.csv')\n",
    "sydney_counts.to_csv('sydney_counts_cleaned_201022.csv')\n",
    "\n",
    "# Data now is cleaned.\n",
    "# Data will be saved.\n",
    "# No outlier detection carried.\n",
    "# Data is assumed to be ok but again we should test for this ahead of final presentation.\n",
    "# If further analysis doesnt obviously make sense will return for outlier detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894e53c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe for NY without weekend.\n",
    "ny_counts = pd.read_csv('ny_counts_weekdays_only_201022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f37406a",
   "metadata": {},
   "source": [
    "## Merging DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83937476",
   "metadata": {},
   "source": [
    "### Concatenate London DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16632340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging London Dataframes first to check output as this is the base Df. \n",
    "london = pd.concat([outer_london, central_london, inner_london])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf67e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Metadata.\n",
    "london.info()\n",
    "\n",
    "# London rows all add up correctly so merge is successful. \n",
    "# Expecting some null values now as inner and central london dfs didnt have gender info.\n",
    "# Expecting some null values now as outer london df didnt have cycle ownership/rental info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb14a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "london.isnull().sum()\n",
    "\n",
    "# All aligns as expected. \n",
    "# Will drop the 6 rows where period and time has null values.\n",
    "# 6 rows being deleted will not impact data with 1.287mio rows\\\n",
    "# and doesnt merit time to investigate these.\n",
    "# Gender & Cycle ownership data was already missing from some of the original data.\n",
    "# The missing rows add up to the sum of the original data where the data was missing\\\n",
    "# so no new duplicates generated.\n",
    "# Confirms concatenation is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c243de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 6 rows where missing values in Time column.\n",
    "london = london.dropna(subset=['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539dca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the Count Data from london with spatial data.\n",
    "# Pull in the Spatial data.\n",
    "# This is a new file where the spatial information from TFL has been coverted.\n",
    "# \"Northing\" & \"Easting\" converted into more universally conventional \"Longitude\" & \"Latitude\".\n",
    "# Reading the new file as London_Biking_sites_reconv.xlsx which is supplied as additional file.\n",
    "bike_site = pd.read_excel(\"London_Biking_sites_reconv.xlsx\")\n",
    "\n",
    "# Contains additional data. \n",
    "# The base TFL supplied data's spatial information has been converted into Longtitude & Latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102dd15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at Metadata.\n",
    "bike_site.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d61e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in bike sites info.\n",
    "duplicateRowsDF_bike_sites = bike_site[bike_site.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979c3b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View output from duplicate check.\n",
    "print(duplicateRowsDF_bike_sites)\n",
    "\n",
    "# No duplicate values in bike sites info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e2991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values.\n",
    "bike_site.isnull().sum()\n",
    "\n",
    "# Two rows with missing values.\n",
    "# Again give size of dataset these can be dropped without further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef39a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Data Sample.\n",
    "bike_site.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming Column on spatial data to align with Count Data.\n",
    "bike_site.rename(columns = {\"UnqID\": \"Site_ID\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4747d45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to remove space in column names.\n",
    "bike_site.columns = bike_site.columns.str.replace(' ','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3552a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 2 missing value rows previously identified.\n",
    "bike_site = bike_site.dropna(subset=['Functional_cycling_area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2cd4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes.\n",
    "dfs = [bike_site, london]\n",
    "london_complete = ft.reduce(lambda left, right: pd.merge(left, right, on='Site_ID'), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in merged Df. \n",
    "# Merged Df created using new library.\n",
    "duplicateRowsDF_london_complete = london_complete[london_complete.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878427be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Metadata of combined df.\n",
    "london_complete.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a3857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Site ID & Prog ID are identical on string test.\n",
    "# Strip integers from SiteID into a column containing just the string values.\n",
    "london_complete['Id_Check']= london_complete['Site_ID'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the stripped strings for match with ProgID.\n",
    "# If no match, output into new column will be FALSE.\n",
    "london_complete['Equality_Test'] = london_complete['Id_Check'].equals(london_complete['ProgID'])\n",
    "\n",
    "# Isolating the different unique values in this column.\n",
    "uniqueValues = london_complete['Equality_Test'].nunique()\n",
    "\n",
    "# Displaying the number of  unique values.\n",
    "print(london_complete[\"Equality_Test\"].unique())\n",
    "\n",
    "# Established that ProgID (which contains strings only) is duplicate of Site_ID.\n",
    "# ProgID can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28bdba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove overlapping spatial data.\n",
    "london_complete = london_complete.drop(['ProgID', \n",
    "                                        'Easting', \n",
    "                                        'Northing', \n",
    "                                        'Location_y', \n",
    "                                        'Id_Check',\n",
    "                                        'Equality_Test'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3adbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all column names to lower case to ease recalling them for analysis.\n",
    "london_complete = london_complete.rename(columns=str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee0edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename some columns to more logical names without changing underlying data dictionary.\n",
    "london_complete = london_complete.rename(columns={'location_x': 'location', \n",
    "                                                  'survey_wave_(year)': 'survey_year'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f29c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows with duplicate values\n",
    "london_complete.drop_duplicates()\n",
    "\n",
    "# All duplicate rows dropped\n",
    "# Returns the number of rows expected\n",
    "# Data now is cleaned\n",
    "# Data will be saved\n",
    "# No outlier detection carried out as data sourced from Tfl and was presented semi wrangled\n",
    "# If further analysis doesnt obviously make sense will return for outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b9e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values in each column\n",
    "for col in london_complete:\n",
    "  print(col,\": \", london_complete[col].nunique())\n",
    "\n",
    "# Survey data over 8 years\n",
    "# Across 1258 counters\n",
    "# Across 1188 streets\n",
    "# Across 33 boroughs in London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38923748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the combined dataframe as a new CSV for backup\n",
    "london_complete.to_csv('london_count_and_site_201022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717769ed",
   "metadata": {},
   "source": [
    "# Introducing Data for Biking Infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbcbce9",
   "metadata": {},
   "source": [
    "Beyond the count data supplied for this project, it will be useful to investigate the cycling infrastructure in London. To this end, the [TfL](https://cycling.data.tfl.gov.uk/ \"Cycling Infrastructure\") produces data on a range of cycling assets especially for the purpose of analysis.\n",
    "\n",
    "Data on the following types of infrastructure are published by the TfL and will be analysed here:\n",
    "\n",
    "- Cycle parking, including the type and capacity of parking\n",
    "- Restricted Points – points where people cycling will have to dismount Paths through parks and other green spaces that can, and cannot, be cycled on\n",
    "- Signage - Signed cycle routes and other wayfinding\n",
    "- Signals - early-release signals at junctions\n",
    "- Traffic calming, including the location of all speed humps in Greater London\n",
    "- Advanced stop lines – boxes at junctions for people cycling\n",
    "- Restricted route - Modal filters and traffic gates which allow cycles to pass but restrict car traffic\n",
    "- Signalised crossings for cycles\n",
    "- Cycle lanes and tracks – including whether they are segregated or painted lanes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c1b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading datapoint CSVs.\n",
    "cycle_park = pd.read_csv('cycle_parking.csv')\n",
    "rct_point = pd.read_csv('restricted_point.csv')\n",
    "signage = pd.read_csv('signage.csv')\n",
    "signal = pd.read_csv('signal.csv')\n",
    "traf_calm = pd.read_csv('traffic_calming.csv')\n",
    "\n",
    "# Reading datalines CSVs.\n",
    "asl = pd.read_csv('advanced_stop_line.csv')\n",
    "rct_route = pd.read_csv('restricted_route.csv')\n",
    "crossing = pd.read_csv('crossing.csv')\n",
    "cyc_lane = pd.read_csv('cycle_lane_track.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b03dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'properties/' from the column names.\n",
    "cycle_park.columns = cycle_park.columns.str.replace('properties/','')\n",
    "rct_point.columns = rct_point.columns.str.replace('properties/','')\n",
    "signage.columns = signage.columns.str.replace('properties/','')\n",
    "signal.columns = signal.columns.str.replace('properties/','')\n",
    "traf_calm.columns = traf_calm.columns.str.replace('properties/','')\n",
    "\n",
    "asl.columns = asl.columns.str.replace('properties/','')\n",
    "rct_route.columns = rct_route.columns.str.replace('properties/','')\n",
    "crossing.columns = crossing.columns.str.replace('properties/','')\n",
    "cyc_lane.columns = cyc_lane.columns.str.replace('properties/','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c3e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'properties.' from the column names.\n",
    "cycle_park.columns = cycle_park.columns.str.replace('properties.','')\n",
    "rct_point.columns = rct_point.columns.str.replace('properties.','')\n",
    "signage.columns = signage.columns.str.replace('properties.','')\n",
    "signal.columns = signal.columns.str.replace('properties.','')\n",
    "traf_calm.columns = traf_calm.columns.str.replace('properties.','')\n",
    "\n",
    "asl.columns = asl.columns.str.replace('properties.','')\n",
    "rct_route.columns = rct_route.columns.str.replace('properties.','')\n",
    "crossing.columns = crossing.columns.str.replace('properties.','')\n",
    "cyc_lane.columns = cyc_lane.columns.str.replace('properties.','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c9fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'geometry/' from the column names.\n",
    "cycle_park.columns = cycle_park.columns.str.replace('geometry/','')\n",
    "rct_point.columns = rct_point.columns.str.replace('geometry/','')\n",
    "signage.columns = signage.columns.str.replace('geometry/','')\n",
    "signal.columns = signal.columns.str.replace('geometry/','')\n",
    "traf_calm.columns = traf_calm.columns.str.replace('geometry/','')\n",
    "\n",
    "asl.columns = asl.columns.str.replace('geometry/','')\n",
    "rct_route.columns = rct_route.columns.str.replace('geometry/','')\n",
    "crossing.columns = crossing.columns.str.replace('geometry/','')\n",
    "cyc_lane.columns = cyc_lane.columns.str.replace('geometry/','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e22f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'geometry.' from the column names.\n",
    "cycle_park.columns = cycle_park.columns.str.replace('geometry.','')\n",
    "rct_point.columns = rct_point.columns.str.replace('geometry.','')\n",
    "signage.columns = signage.columns.str.replace('geometry.','')\n",
    "signal.columns = signal.columns.str.replace('geometry.','')\n",
    "traf_calm.columns = traf_calm.columns.str.replace('geometry.','')\n",
    "\n",
    "asl.columns = asl.columns.str.replace('geometry.','')\n",
    "rct_route.columns = rct_route.columns.str.replace('geometry.','')\n",
    "crossing.columns = crossing.columns.str.replace('geometry.','')\n",
    "cyc_lane.columns = cyc_lane.columns.str.replace('geometry.','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75272c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing column names to all lower.\n",
    "cycle_park = cycle_park.rename(columns=str.lower)\n",
    "rct_point = rct_point.rename(columns=str.lower)\n",
    "signage = signage.rename(columns=str.lower)\n",
    "signal = signal.rename(columns=str.lower)\n",
    "traf_calm = traf_calm.rename(columns=str.lower)\n",
    "\n",
    "asl = asl.rename(columns=str.lower)\n",
    "rct_route = rct_route.rename(columns=str.lower)\n",
    "crossing = crossing.rename(columns=str.lower)\n",
    "cyc_lane = cyc_lane.rename(columns=str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83ece41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unneccesary columns and renaming columns where necessary.\n",
    "# Extracting latitude and Longitude from a combined column.\n",
    "\n",
    "splits = signage[\"coordinates\"].str.split(\",\", n = 1, expand = True)\n",
    "signage[\"longitude\"] = splits[0]\n",
    "signage[\"latitude\"] = splits[1]\n",
    "signage = signage.drop(columns=[\"coordinates\"])\n",
    "signage[\"longitude\"] = signage[\"longitude\"].str.strip('[').astype(float)\n",
    "signage[\"latitude\"] = signage[\"latitude\"].str.strip(']').astype(float)\n",
    "\n",
    "signage = signage.drop(columns=['unnamed: 0','type','type','ss_routen','ss_access', 'photo1_url', 'photo2_url'])\n",
    "signage = signage.rename(columns={'ss_road':'Road Marking',\n",
    "                                  'ss_patch':'Coloured Patch on Surface',\n",
    "                                  'ss_colour':'Colour of Patch',\n",
    "                                  'ss_facing':'Facing Off-side',\n",
    "                                  'ss_nocyc':'No Cycling',\n",
    "                                  'ss_noveh':'No Vehicles',\n",
    "                                  'ss_circ':'Circular Sign',\n",
    "                                  'ss_exempt':'Exemption',\n",
    "                                  'ss_noleft':'No Left Turn Exception',\n",
    "                                  'ss_norigh':'No Right Turn Exception',\n",
    "                                  'ss_left':'Compulsory Turn Left Exception',\n",
    "                                  'ss_right':'Compulsory Turn Right exception',\n",
    "                                  'ss_noexce':'No Straight Ahead Exception',\n",
    "                                  'ss_dismou':'Cyclists Dismount',\n",
    "                                  'ss_end':'End of Route',\n",
    "                                  'ss_cycsmb':'Cycle Symbol',\n",
    "                                 })\n",
    "\n",
    "\n",
    "# Restricted point dataframe.\n",
    "rct_point = rct_point.drop(columns=['type','type','photo1_url', 'photo2_url'])\n",
    "rct_point = rct_point.rename(columns={'coordinates/0': 'longitude', \n",
    "                                      'coordinates/1': 'latitude',\n",
    "                                     'rst_steps':'Steps',\n",
    "                                      'rst_lift':'Lift',\n",
    "                                      'res_pedest':'Pedestrian-Only Route',\n",
    "                                      'res_bridge':'Pedestrian Bridge',\n",
    "                                      'res_tunnel':'Pedestrian Tunnel',\n",
    "                                      'res_steps':'Steps','res_lift':'Lift'\n",
    "                                     })\n",
    "\n",
    "# Cycle Park dataframe.\n",
    "cycle_park = cycle_park.drop(columns=['type','type','photo1_url', 'photo2_url'])\n",
    "cycle_park = cycle_park.rename(columns={'coordinates/0': 'longitude', \n",
    "                                        'coordinates/1': 'latitude', \n",
    "                                        'prk_carr':'Carriageway',\n",
    "                                        'prk_cover':'Covered',\n",
    "                                        'prk_secure':'Secure',\n",
    "                                        'prk_locker':'Locker',\n",
    "                                        'prk_sheff':'Sheffield Stand',\n",
    "                                        'prk_mstand':'\"M\" stand',\n",
    "                                        'prk_pstand':'\"P\" stand',\n",
    "                                        'prk_hoop':'Cyclehoop',\n",
    "                                        'prk_post':'Post',\n",
    "                                        'prk_buterf':'Butterfly',\n",
    "                                        'prk_wheel':'Wheel Rack',\n",
    "                                        'prk_hangar':'Bike Hangar',\n",
    "                                        'prk_tier':'Two Tier',\n",
    "                                        'prk_other':'Other',\n",
    "                                        'prk_provis':'Provision',\n",
    "                                        'prk_cpt':'Capacity','':''\n",
    "                                        })\n",
    "\n",
    "\n",
    "# Signal dataframe.\n",
    "signal = signal.drop(columns=['type','type','photo1_url', 'photo2_url'])\n",
    "signal = signal.rename(columns={'coordinates/0': 'longitude', \n",
    "                                'coordinates/1': 'latitude',\n",
    "                               'sig_head':'Cycle Signal Head',\n",
    "                                'sig_separa':'Separate Stage for Cyclists',\n",
    "                                'sig_early':'Early Release',\n",
    "                                'sig_twostg':'Two Stage Turn',\n",
    "                                'sig_gate':'Cycle/Bus Gate'\n",
    "                               })\n",
    "\n",
    "\n",
    "\n",
    "# Traffic calming dataframe.\n",
    "\n",
    "splits2 = traf_calm[\"coordinates\"].str.split(\",\", n = 1, expand = True)\n",
    "traf_calm[\"longitude\"] = splits2[0]\n",
    "traf_calm[\"latitude\"] = splits2[1]\n",
    "traf_calm = traf_calm.drop(columns=[\"coordinates\"])\n",
    "traf_calm[\"longitude\"] = traf_calm[\"longitude\"].str.strip('[').astype(float)\n",
    "traf_calm[\"latitude\"] = traf_calm[\"latitude\"].str.strip(']').astype(float)\n",
    "\n",
    "\n",
    "traf_calm = traf_calm.drop(columns=['unnamed: 0','type','type','photo1_url', 'photo2_url'])\n",
    "signal = signal.rename(columns={'trf_raised':'Raised Table',\n",
    "                                'trf_entry':'Side Road Entry Treatment',\n",
    "                                'trf_cushi':'Speed Cushions',\n",
    "                                'trf_hump':'Speed Hump','trf_sinuso':'Sinusoidal',\n",
    "                                'trf_barier':'Barrier',\n",
    "                                'trf_narow':'Carriageway Narrowing',\n",
    "                                'trf_calm':'Other'\n",
    "                                })\n",
    "\n",
    "\n",
    "# Advanced stop lanes dataframe.\n",
    "asl = asl.drop(asl.iloc[:, 14:31], axis = 1)\n",
    "asl = asl.drop(asl.iloc[:, 0:1], axis = 1)\n",
    "asl = asl.rename(columns={'coordinates/0/0': 'longitude_start', \n",
    "                          'coordinates/0/1': 'latitude_start', \n",
    "                          'coordinates/1/0': 'longitude_end',\n",
    "                          'coordinates/1/1': 'latitude_end',\n",
    "                          'asl_fdr':'Feeder Lane',\n",
    "                          'asl_fdrlft':'Feeder Lane on Left',\n",
    "                          'asl_fdcent':'Feeder Lane in Centre',\n",
    "                          'asl_fdrigh':'Feeder Lane on Right',\n",
    "                          'asl_shared':'Shared Nearside Lane',\n",
    "                          'asl_colour':'Colour'})\n",
    "\n",
    "\n",
    "# Restricted Route dataframe.\n",
    "\n",
    "splits = rct_route[\"coordinates\"].str.split(\",\", n = 1, expand = True)\n",
    "rct_route[\"longitude\"] = splits[0]\n",
    "rct_route[\"latitude\"] = splits[1]\n",
    "rct_route = rct_route.drop(columns=[\"coordinates\"])\n",
    "rct_route[\"longitude\"] = rct_route[\"longitude\"].str.strip('[').astype(float)\n",
    "rct_route[\"latitude\"] = rct_route[\"latitude\"].str.strip(']').astype(float)\n",
    "rct_route = rct_route.drop(['unnamed: 0','type','type','photo1_url','photo2_url'], axis = 1)\n",
    "\n",
    "# Crossing dataframe.\n",
    "crossing = crossing.drop(crossing.iloc[:, 14:56], axis = 1)\n",
    "crossing = crossing.drop(crossing.iloc[:, 0:1], axis = 1)\n",
    "crossing = crossing.rename(columns={'coordinates/0/0': 'longitude_start', \n",
    "                          'coordinates/0/1': 'latitude_start', \n",
    "                          'coordinates/1/0': 'longitude_end',\n",
    "                          'coordinates/1/1': 'latitude_end',\n",
    "                          'crs_signal':'Signal-Controlled Crossing',\n",
    "                          'crs_segreg':'Segregated Cycles and Pedestrians',\n",
    "                          'crs_cygap':'Cycle Gap',\n",
    "                          'crs_pedest':'Pedestrian-Only Crossing',\n",
    "                          'crs_level':'Level Crossing'\n",
    "                                   })\n",
    "\n",
    "\n",
    "# Cycle Lane dataframe.\n",
    "\n",
    "splits = cyc_lane[\"coordinates\"].str.split(\",\", n = 2, expand = True)\n",
    "cyc_lane[\"longitude\"] = splits[0]\n",
    "cyc_lane[\"latitude\"] = splits[1]\n",
    "cyc_lane = cyc_lane.drop(columns=[\"coordinates\"])\n",
    "cyc_lane[\"longitude\"] = cyc_lane[\"longitude\"].str.strip('[').astype(float)\n",
    "cyc_lane[\"latitude\"] = cyc_lane[\"latitude\"].str.strip(']').astype(float)\n",
    "\n",
    "\n",
    "cyc_lane = cyc_lane.drop(['unnamed: 0','type','type','photo1_url','photo2_url'], axis = 1)\n",
    "cyc_lane = cyc_lane.rename(columns={'clt_carr':'On-off Carriageway',\n",
    "                                    'clt_segreg':'Segregated Lane/Track',\n",
    "                                    'clt_stepp':'Stepped Lane/Track',\n",
    "                                    'clt_parseg':'Partially Segregated Lane/Track',\n",
    "                                    'clt_shared':'Shared Lane or Footway',\n",
    "                                    'clt_mandat':'Mandatory Cycle Lane',\n",
    "                                    'clt_advis':'Advisory Cycle Lane',\n",
    "                                    'clt_priori':'Cycle Lane/Track Priority',\n",
    "                                    'clt_contra':'Contraflow Lane/Track',\n",
    "                                    'clt_bidire':'Bi-directional',\n",
    "                                    'clt_cbypas':'Cycle Bypass',\n",
    "                                    'clt_bbypas':'Continuous Cycle Facilities at Bus Stop',\n",
    "                                    'clt_parkr':'Park Route','clt_waterr':'Waterside Route',\n",
    "                                    'clt_ptime':'Part-time (if true) or Full-time (if false)',\n",
    "                                    'clt_access':'Access Times','clt_colour':'Colour'\n",
    "                                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f96252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing null values.\n",
    "cyc_lane = cyc_lane.fillna(0)\n",
    "rct_route = rct_route.fillna(0)\n",
    "signage = signage.fillna(0)\n",
    "traf_calm = traf_calm.fillna(0)\n",
    "crossing = crossing.fillna(0)\n",
    "asl = asl.fillna(0)\n",
    "signal = signal.fillna(0)\n",
    "cycle_park = cycle_park.fillna(0)\n",
    "rct_point = rct_point.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse London Df Dates into appropriate format.\n",
    "\n",
    "# Convert to datetime format.\n",
    "cyc_lane['svdate'] = pd.to_datetime(cyc_lane['svdate'], dayfirst=True, yearfirst=False, errors = 'coerce')\n",
    "rct_route['svdate'] = pd.to_datetime(rct_route['svdate'], dayfirst=True, yearfirst=False , errors = 'coerce')\n",
    "signage['svdate'] = pd.to_datetime(signage['svdate'], dayfirst=True, yearfirst=False, errors = 'coerce')\n",
    "traf_calm['svdate'] = pd.to_datetime(traf_calm['svdate'], dayfirst=True, yearfirst=False , errors = 'coerce')\n",
    "crossing['svdate'] = pd.to_datetime(crossing['svdate'], dayfirst=True, yearfirst=False, errors = 'coerce')\n",
    "asl['svdate'] = pd.to_datetime(asl['svdate'], dayfirst=True, yearfirst=False, errors = 'coerce')\n",
    "signal['svdate'] = pd.to_datetime(signal['svdate'], dayfirst=True, yearfirst=False, errors = 'coerce')\n",
    "cycle_park['svdate'] = pd.to_datetime(cycle_park['svdate'], dayfirst=True, yearfirst=False, errors = 'coerce')\n",
    "rct_point['svdate'] = pd.to_datetime(rct_point['svdate'], dayfirst=True, yearfirst=False, errors = 'coerce')\n",
    "\n",
    "# Sort DataFrame by chronological order.\n",
    "cyc_lane = cyc_lane.sort_values(['svdate'])\n",
    "rct_route = rct_route.sort_values(['svdate'])\n",
    "signage = signage.sort_values(['svdate'])\n",
    "traf_calm = traf_calm.sort_values(['svdate'])\n",
    "crossing = crossing.sort_values(['svdate'])\n",
    "asl = asl.sort_values(['svdate'])\n",
    "signal = signal.sort_values(['svdate'])\n",
    "cycle_park = cycle_park.sort_values(['svdate'])\n",
    "rct_point = rct_point.sort_values(['svdate'])\n",
    "\n",
    "\n",
    "# Define day of the week in English and add back as a column.\n",
    "cyc_lane['Day_of_week'] = cyc_lane['svdate'].dt.day_name()\n",
    "rct_route['Day_of_week'] = rct_route['svdate'].dt.day_name()\n",
    "signage['Day_of_week'] = signage['svdate'].dt.day_name()\n",
    "traf_calm['Day_of_week'] = traf_calm['svdate'].dt.day_name()\n",
    "crossing['Day_of_week'] = crossing['svdate'].dt.day_name()\n",
    "asl['Day_of_week'] = asl['svdate'].dt.day_name()\n",
    "signal['Day_of_week'] = signal['svdate'].dt.day_name()\n",
    "cycle_park['Day_of_week'] = cycle_park['svdate'].dt.day_name()\n",
    "rct_point['Day_of_week'] = rct_point['svdate'].dt.day_name()\n",
    "\n",
    "# Pass Month into a new column.\n",
    "cyc_lane['month'] = cyc_lane['svdate'].dt.month\n",
    "rct_route['month'] = rct_route['svdate'].dt.month\n",
    "signage['month'] = signage['svdate'].dt.month\n",
    "traf_calm['month'] = traf_calm['svdate'].dt.month\n",
    "crossing['month'] = crossing['svdate'].dt.month\n",
    "asl['month'] = asl['svdate'].dt.month\n",
    "signal['month'] = signal['svdate'].dt.month\n",
    "cycle_park['month'] = cycle_park['svdate'].dt.month\n",
    "rct_point['month'] = rct_point['svdate'].dt.month\n",
    "\n",
    "# Pass Year into a new column.\n",
    "cyc_lane['year'] = cyc_lane['svdate'].dt.year\n",
    "rct_route['year'] = rct_route['svdate'].dt.year\n",
    "signage['year'] = signage['svdate'].dt.year\n",
    "traf_calm['year'] = traf_calm['svdate'].dt.year\n",
    "crossing['year'] = crossing['svdate'].dt.year\n",
    "asl['year'] = asl['svdate'].dt.year\n",
    "signal['year'] = signal['svdate'].dt.year\n",
    "cycle_park['year'] = cycle_park['svdate'].dt.year\n",
    "rct_point['year'] = rct_point['svdate'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa3873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting boolean values to integers to sum the infrastructure.\n",
    "cyc_lane.replace({False: 0, True: 1}, inplace=True)\n",
    "rct_route.replace({False: 0, True: 1}, inplace=True)\n",
    "signage.replace({False: 0, True: 1}, inplace=True)\n",
    "traf_calm.replace({False: 0, True: 1}, inplace=True)\n",
    "crossing.replace({False: 0, True: 1}, inplace=True)\n",
    "asl.replace({False: 0, True: 1}, inplace=True)\n",
    "signal.replace({False: 0, True: 1}, inplace=True)\n",
    "cycle_park.replace({False: 0, True: 1}, inplace=True)\n",
    "rct_point.replace({False: 0, True: 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d53f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "rct_point.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding calculated column to sum up all infrastructure.\n",
    "cyc_lane['infra_sum'] = cyc_lane.iloc[:, 2:17].sum(axis=1)\n",
    "rct_route['infra_sum'] = rct_route.iloc[:, 2:32].sum(axis=1)\n",
    "signage['infra_sum'] = signage.iloc[:, 2:29].sum(axis=1)\n",
    "traf_calm['infra_sum'] = traf_calm.iloc[:, 2:9].sum(axis=1)\n",
    "crossing['infra_sum']= crossing.iloc[:, 6:10].sum(axis=1)\n",
    "asl['infra_sum']= asl.iloc[:, 6:10].sum(axis=1)\n",
    "signal['infra_sum']= signal.iloc[:, 4:8].sum(axis=1)\n",
    "cycle_park['infra_sum'] = cycle_park.iloc[:, 4:19].sum(axis=1)\n",
    "rct_point['infra_sum'] = rct_point.iloc[:, 4:5].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe0cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a time series grouping by year.\n",
    "# Time series is sum of all infrastructure per type of infrastructure.\n",
    "cyc_lane_grpby = cyc_lane.groupby(['year'], as_index=False)['infra_sum'].sum()\n",
    "rct_route_grpby = rct_route.groupby(['year'], as_index=False)['infra_sum'].sum()\n",
    "signage_grpby = signage.groupby(['year'], as_index=False)['infra_sum'].sum()\n",
    "crossing_grpby = crossing.groupby(['year'], as_index=False)['infra_sum'].sum()\n",
    "asl_grpby = asl.groupby(['year'], as_index=False)['infra_sum'].sum()\n",
    "signal_grpby = signal.groupby(['year'], as_index=False)['infra_sum'].sum()\n",
    "cycle_park_grpby = cycle_park.groupby(['year'], as_index=False)['infra_sum'].sum()\n",
    "rct_point_grpby = rct_point.groupby(['year'], as_index=False)['infra_sum'].sum()\n",
    "traf_calm_grpby = traf_calm.groupby(['year'], as_index=False)['infra_sum'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d80336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the infrastructure data by borough only.\n",
    "cyc_lane_bor_grpby = cyc_lane.groupby(['borough'], as_index=False)['infra_sum'].sum()\n",
    "rct_route_bor_grpby = rct_route.groupby(['borough'], as_index=False)['infra_sum'].sum()\n",
    "signage_bor_grpby = signage.groupby(['borough'], as_index=False)['infra_sum'].sum()\n",
    "crossing_bor_grpby = crossing.groupby(['borough'], as_index=False)['infra_sum'].sum()\n",
    "signal_bor_grpby = signal.groupby(['borough'], as_index=False)['infra_sum'].sum()\n",
    "cycle_park_bor_grpby = cycle_park.groupby(['borough'], as_index=False)['infra_sum'].sum()\n",
    "rct_point_bor_grpby = rct_point.groupby(['borough'], as_index=False)['infra_sum'].sum()\n",
    "traf_calm_bor_grpby = traf_calm.groupby(['borough'], as_index=False)['infra_sum'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccebd170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking down the infrastructure by borough & year.\n",
    "cyc_lane_yr_bor_grpby = cyc_lane.groupby(['year','borough'], as_index=False)['infra_sum'].sum()\n",
    "rct_route_yr_bor_grpby = rct_route.groupby(['year','borough'], as_index=False)['infra_sum'].sum()\n",
    "signage_yr_bor_grpby = signage.groupby(['year','borough'], as_index=False)['infra_sum'].sum()\n",
    "crossing_yr_bor_grpby = crossing.groupby(['year','borough'], as_index=False)['infra_sum'].sum()\n",
    "\n",
    "signal_yr_bor_grpby = signal.groupby(['year','borough'], as_index=False)['infra_sum'].sum()\n",
    "cycle_park_yr_bor_grpby = cycle_park.groupby(['year','borough'], as_index=False)['infra_sum'].sum()\n",
    "rct_point_yr_bor_grpby = rct_point.groupby(['year','borough'], as_index=False)['infra_sum'].sum()\n",
    "traf_calm_yr_bor_grpby = traf_calm.groupby(['year','borough'], as_index=False)['infra_sum'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e489ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the top 10 boroughs with the most cycle lanes.\n",
    "cyc_lane_largest_ten = cyc_lane_bor_grpby.nlargest(10,'infra_sum')\n",
    "\n",
    "# Plotting.\n",
    "cyc_lane_largest_ten.plot.barh(x = \"borough\", \n",
    "                               y = \"infra_sum\", \n",
    "                               title = \"Top 10 boroughs for highest number of Cycle Lanes\", \n",
    "                               legend = False, \n",
    "                               xlabel = 'Borough', \n",
    "                               ylabel = 'No. of Cycle Lanes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa46dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the top 10 boroughs with the most Restricted Routes.\n",
    "rct_route_largest_ten = rct_route_bor_grpby.nlargest(10, 'infra_sum')\n",
    "\n",
    "# Plotting.\n",
    "\n",
    "rct_route_largest_ten.plot.barh(x=\"borough\", \n",
    "                                y=\"infra_sum\", \n",
    "                                title=\"Top 10 boroughs for highest number of  Restricted Routes\", \n",
    "                                legend=False, \n",
    "                                xlabel='Borough', \n",
    "                                ylabel= 'No. of Restricted Routes' )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f42e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the top 10 boroughs with the most signage.\n",
    "signage_bor_largest_ten = signage_bor_grpby.nlargest(10,'infra_sum')\n",
    "\n",
    "# Plotting.\n",
    "signage_bor_largest_ten.plot.barh(x=\"borough\", \n",
    "                                  y=\"infra_sum\", \n",
    "                                  title=\"Top 10 boroughs for highest number of Signages\", \n",
    "                                  legend=False, \n",
    "                                  xlabel='Borough', \n",
    "                                  ylabel= 'No. of Signages')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b0f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the top 5 boroughs with the most signalised crossings for cycles.\n",
    "crossing_bor_largest_ten = crossing_bor_grpby.nlargest(10,'infra_sum')\n",
    "\n",
    "# Plotting.\n",
    "crossing_bor_largest_ten.plot.barh(x=\"borough\", \n",
    "                                   y=\"infra_sum\", \n",
    "                                   title=\"Top 10 boroughs for highest number of Signalised Crossings\", \n",
    "                                   legend=False, \n",
    "                                   xlabel='Borough', \n",
    "                                   ylabel='No. of Signals' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d304a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the top 10 boroughs with the most Early release signals at junctions for cyclists.\n",
    "signal_bor_largest_ten = signal_bor_grpby.nlargest(10,'infra_sum')\n",
    "\n",
    "# Plotting.\n",
    "signal_bor_largest_ten.plot.barh(x=\"borough\", \n",
    "                                 y=\"infra_sum\", \n",
    "                                 title=\"Top 10 boroughs for highest number of Early Release Signals\", \n",
    "                                 legend=False, \n",
    "                                 xlabel='Borough', \n",
    "                                 ylabel='No. of Signals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0884d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the top 10 boroughs with the most cycle parking.\n",
    "cycle_park_bor_largest_ten = cycle_park_bor_grpby.nlargest(10,'infra_sum')\n",
    "\n",
    "# Plotting.\n",
    "cycle_park_bor_largest_ten.plot.barh(x=\"borough\", \n",
    "                                     y=\"infra_sum\", \n",
    "                                     title=\"Top 10 boroughs for highest number of Cycle Parkings\", \n",
    "                                     legend=False, \n",
    "                                     xlabel='borough', \n",
    "                                     ylabel='No. of Cycle Parkings' )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feae970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the top 10 boroughs with the most restricted points.\n",
    "rct_point_bor_largest_ten = rct_point_bor_grpby.nlargest(10,'infra_sum')\n",
    "\n",
    "# Plotting.\n",
    "rct_point_bor_largest_ten.plot.barh(x=\"borough\", \n",
    "                                    y=\"infra_sum\", \n",
    "                                    title=\"Top 10 boroughs with highest number of Restricted Points\", \n",
    "                                    legend=False, \n",
    "                                    xlabel='borough',\n",
    "                                    ylabel='No. of Restricted points' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f7f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the top 10 boroughs with the most Traffic Calming.\n",
    "traf_calm_bor_largest_ten = traf_calm_bor_grpby.nlargest(10, 'infra_sum')\n",
    "\n",
    "# Plotting.\n",
    "traf_calm_bor_largest_ten.plot.barh(x=\"borough\", \n",
    "                                    y=\"infra_sum\", \n",
    "                                    title=\"Top 10 for Traffic Calming\", \n",
    "                                    legend=False, \n",
    "                                    xlabel='borough', \n",
    "                                    ylabel='No. of Traffic Calming Measures' )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dd9871",
   "metadata": {},
   "source": [
    "# Introducing additional Data from independent research\n",
    "\n",
    "Absent timeseries data covering the full period of this analysis on the development of cycling infrastructure in London, some other variables will be analysed which could have a meaningfully quantifiable relationship with cycling uptake. \n",
    "\n",
    "- [Private Car Ownership](https://data.london.gov.uk/dataset/licensed-vehicles-numbers-borough \"Private Car Ownership\") \n",
    "- [Traffic Flow](https://data.london.gov.uk/dataset/traffic-flows-borough \"Traffic Flow\")\n",
    "\n",
    "Furthermore, to baseline and contextualise the cycling count data, population time series for [London](https://www.macrotrends.net/cities/22860/london/population \"London\"), [New York](https://www.macrotrends.net/states/new-york/population \"New York\") and [Sydney](https://www.macrotrends.net/cities/206167/sydney/population \"Sydney\") are also being introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f51df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce new data.\n",
    "# Traffic and Car Ownership data.\n",
    "car_own = pd.read_csv('private_cars_london.csv')\n",
    "traffic_flow = pd.read_csv('traffic_flow_borough.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf946d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population & cycling data.\n",
    "pop_london = pd.read_csv('London_population.csv')\n",
    "pop_ny = pd.read_csv('New_York_population.csv')\n",
    "pop_sydney = pd.read_csv('Sydney_population.csv')\n",
    "london_complete = pd.read_csv('london_count_and_site_201022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all column names to lower case to ease recalling them for analysis.\n",
    "car_own = car_own.rename(columns=str.lower)\n",
    "traffic_flow = traffic_flow.rename(columns=str.lower)\n",
    "pop_london = pop_london.rename(columns=str.lower)\n",
    "pop_ny = pop_ny.rename(columns=str.lower)\n",
    "pop_sydney = pop_sydney.rename(columns=str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfe1628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to remove space in column names.\n",
    "traffic_flow.columns = traffic_flow.columns.str.replace(' ','_')\n",
    "car_own.columns = car_own.columns.str.replace(' ','_')\n",
    "pop_london.columns = pop_london.columns.str.replace(' ','_')\n",
    "pop_ny.columns = pop_ny.columns.str.replace(' ','_')\n",
    "pop_sydney.columns = pop_sydney.columns.str.replace(' ','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb55a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Sample.\n",
    "pop_london.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70086683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Sample.\n",
    "pop_ny.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81307607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Sample.\n",
    "pop_sydney.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1b94c0",
   "metadata": {},
   "source": [
    "## Shaping and cleaning the data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414dad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to datatime format.\n",
    "pop_london['date'] = pd.to_datetime(pop_london['date'], format='%d/%m/%Y')\n",
    "pop_ny['date'] = pd.to_datetime(pop_ny['date'], format='%d/%m/%Y')\n",
    "pop_sydney['date'] = pd.to_datetime(pop_sydney['date'], format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ad473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to create a Year Column in each DF.\n",
    "# Will use this to merge the population data.\n",
    "pop_london['year'] = pop_london['date'].dt.year\n",
    "pop_ny['year'] = pop_ny['date'].dt.year\n",
    "pop_sydney['year'] = pop_sydney['date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f5c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop date column from each population DF.\n",
    "pop_london.drop('date', axis=1, inplace=True)\n",
    "pop_ny.drop('date', axis=1, inplace=True)\n",
    "pop_sydney.drop('date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c54cfac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View Sample.\n",
    "car_own.head(5)\n",
    "\n",
    "# Shows absolute numbers of cars registered in private ownership over time in each borough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6134ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Sample.\n",
    "traffic_flow.head(5)\n",
    "\n",
    "# Unit of measure is in millions & kilometers.\n",
    "# Shows million vehicle kilometers travelled by all cars through each borough over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the population dfs.\n",
    "# Merge the traffic flow and car ownership details.\n",
    "dfs = [pop_london, pop_ny, pop_sydney]\n",
    "all_pop = ft.reduce(lambda left, right: pd.merge(left, right, on='year'), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801810d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View output.\n",
    "all_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e4e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Columns. \n",
    "all_pop.rename(columns={'_population_x': 'london_pop', '_population_y': 'ny_pop', \n",
    "                       '_population': 'sydney_pop'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b0b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column showing Y-o-Y % change in population for each city.\n",
    "all_pop['ldn_pop_change'] = pd.DataFrame.pct_change(all_pop['london_pop'])*100\n",
    "all_pop['ny_pop_change'] = pd.DataFrame.pct_change(all_pop['ny_pop'])*100\n",
    "all_pop['sydney_pop_change'] = pd.DataFrame.pct_change(all_pop['sydney_pop'])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53d6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log values for population to reduce the scalar impact on visualisations.\n",
    "all_pop['log_london_pop'] = np.log(all_pop['london_pop'])\n",
    "all_pop['log_ny_pop'] = np.log(all_pop['ny_pop'])\n",
    "all_pop['log_sydney_pop'] = np.log(all_pop['sydney_pop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0d6af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape the df for further analysis.\n",
    "all_pop1 = pd.melt(all_pop, id_vars =['year'], value_vars =['log_london_pop', 'log_ny_pop', 'log_sydney_pop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5414f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename some columns to align across analysis.\n",
    "all_pop1 = all_pop1.rename(columns={'variable': 'location', 'value': 'population(log)', \n",
    "                                   'population': 'location'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec65cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace column values to align analysis.\n",
    "all_pop1 = all_pop1.replace(['log_london_pop','log_ny_pop',\n",
    "                             'log_sydney_pop'],['london', 'new_york', 'sydney'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea02c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look now at how cycling uptake has evolved in London over the same period.\n",
    "# Introduce the merged london data set and recall the metadata.\n",
    "london_complete.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f96029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look now at how cycling uptake has evolved in London over the same period.\n",
    "# Ensure date is in appropriate format.\n",
    "london_complete['survey_date'] = pd.to_datetime(london_complete.survey_date)\n",
    "\n",
    "# Aggregate by year.\n",
    "# Insert column year to run aggregation function.\n",
    "london_complete['year'] = london_complete['survey_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc84a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all NaN values with 0.\n",
    "london_complete.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4721775",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Simplify df columns.\n",
    "london_complete=london_complete.drop(['Unnamed: 0', 'survey_year', 'direction', \n",
    "                                      'surveydescription', 'total_cycles'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976970a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a total count in outer london.\n",
    "london_complete['total_outer'] = london_complete['number_of_male_cycles']+london_complete['number_of_female_cycles']+london_complete['number_of_unknown_cycles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb493e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a total count for inner and central london.\n",
    "london_complete['total_inn_cen'] = london_complete['number_of_private_cycles']+london_complete['number_of_cycle_hire_bikes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc12a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all NaN values with 0.\n",
    "london_complete['total_outer'] = london_complete['total_outer'].fillna(0)\n",
    "london_complete['total_inn_cen'] = london_complete['total_inn_cen'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265328cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a global total uptake column for cycling counts in all areas of london.\n",
    "london_complete['total_uptake'] = london_complete['total_outer']+london_complete['total_inn_cen'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455eec0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group by year to see change of total cycling uptake YoY.\n",
    "df= london_complete.groupby(['year'], as_index=False)['total_uptake'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ad4922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all numbers on log scale to supress scalar affect.\n",
    "df['log_total_uptake_ldn'] = np.log(df['total_uptake']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0370e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the df.\n",
    "df=df.drop(['total_uptake'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a73952",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Granularity on how total_uptake evolved in the 3 parts of London across common variables.\n",
    "df1= london_complete.groupby(['year', 'functional_cycling_area', 'weather', 'period', \n",
    "                             'day_of_week', 'month_name', 'month', 'season'], as_index=False)['total_uptake'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ecbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the changes.\n",
    "# Put all numbers on log scale to supress scalar affect.\n",
    "df1['log_total_uptake_ldn'] = np.log(df1['total_uptake']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddcbd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get overview of how cycling uptake has progressed over the years in New York.\n",
    "df3 = ny_counts.groupby(['year'], as_index=False)['counts'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aaa275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all numbers on log scale to supress scalar affects.\n",
    "df3['log_total_uptake_ny'] = np.log(df3['counts']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88654399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the df.\n",
    "df3=df3.drop(['counts'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f19e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all column names to lower case to ease recalling them for analysis.\n",
    "sydney_counts = sydney_counts.rename(columns=str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27577a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get overview of how cycling uptake has progressed over the years in Sydney.\n",
    "df4 = sydney_counts.groupby(['year'], as_index=False)['totalcount'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2775aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all numbers on log scale to supress scalar affects.\n",
    "df4['log_total_uptake_sydney'] = np.log(df4['totalcount']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8509ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Df4 shows strange dip in 2018 numbers.\n",
    "# Investigate further.\n",
    "df4a = sydney_counts.groupby([\"year\",\"month\"], as_index=False)['totalcount'].sum()\n",
    "df4a\n",
    "\n",
    "# March 2018 data missing\n",
    "# Oct 2018 has duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43532d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate.\n",
    "df4a = df4a.drop([17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b41ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for correction of missing values.\n",
    "df4a = df4a.pivot(index='year', columns='month', values='totalcount')\n",
    "df4a.as_index=False\n",
    "# This inserts March."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb96a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing value with interpolated value. \n",
    "df4a = df4a.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2797aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the dataframe for further analysis and alignment with other cities.\n",
    "df4a['totalcount'] = df4a['March'] + df4a[\"October\"]\n",
    "df4a['log_total_uptake_sydney'] = np.log(df4a['totalcount'])\n",
    "df4a=df4a.drop(['March', 'October', 'totalcount'], axis=1)\n",
    "\n",
    "# Reset the index.\n",
    "df4 = df4a.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca035e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all regions/cities of interest in one chart.\n",
    "# Merge the dataframes.\n",
    "dfs = [df, df3, df4]\n",
    "on = ['year']\n",
    "all_counts = ft.reduce(lambda left, right: pd.merge(left, right, on= on), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371decb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Melt the dataframe to plot all three cities into one chart.\n",
    "df5 = pd.melt(all_counts, id_vars =['year'], value_vars =['log_total_uptake_ldn', \n",
    "                                                          'log_total_uptake_ny', \n",
    "                                                          'log_total_uptake_sydney'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e98cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename some columns to align across analysis\n",
    "df5 = df5.rename(columns={'variable': 'location', 'value': 'cycling_uptake_log'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a947f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace location values to align across analysis\n",
    "df5 = df5.replace(['log_total_uptake_ldn','log_total_uptake_ny',\n",
    "                   'log_total_uptake_sydney'],['london', 'new_york', 'sydney'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47faca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Population & Cycling Uptake Data across all 3 cities over the period being analysed\n",
    "dfs = [df5, all_pop1]\n",
    "on = ['year', 'location']\n",
    "city_counts = ft.reduce(lambda left, right: pd.merge(left, right, on=on), dfs)\n",
    "\n",
    "# View output\n",
    "city_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff47505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to remove brackets\n",
    "city_counts = city_counts.rename(columns={'population(log)': 'population_log'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f54b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new pivot for cycling uptake\n",
    "# This will help visualise the cycling and population values in one chart\n",
    "city_counts_piv_CU = city_counts.pivot(index='year', columns='location', values='cycling_uptake_log')\n",
    "city_counts_piv_CU = city_counts_piv_CU.reset_index()\n",
    "\n",
    "#View output\n",
    "city_counts_piv_CU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3722c1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for population\n",
    "city_counts_piv_pop = city_counts.pivot(index= 'year', columns= 'location', values='population_log')\n",
    "city_counts_piv_pop = city_counts_piv_pop.reset_index()\n",
    "\n",
    "#View output\n",
    "city_counts_piv_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6938e8ab",
   "metadata": {},
   "source": [
    "# Visual Insights from the quantitative data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620e25c0",
   "metadata": {},
   "source": [
    "## Comparative Analysis between Cities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1bfa5e",
   "metadata": {},
   "source": [
    "### How have population & cycling uptake changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e2e726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise if cycling uptake in each city is proportionate to its population.\n",
    "fig, ax1 = plt.subplots()\n",
    "fig.set_size_inches(10, 5)\n",
    "\n",
    "# twinx() function is required to have two 'y-axis', for two 'x-axis' we will use twiny() function.\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plotting both DFs.\n",
    "city_counts_piv_CU.plot(x='year',y=['london','new_york','sydney'] ,marker='o', ax=ax1)\n",
    "city_counts_piv_pop.plot(x='year',y=['london','new_york','sydney'], linestyle='dashed', ax=ax2)\n",
    "\n",
    "# Plotting attributes.\n",
    "ax1.set_title('Comparison of Population vs Cycling Uptake')\n",
    "ax1.set_xlabel('Year')\n",
    "ax1.set_ylabel('Population')\n",
    "ax2.set_ylabel('Cycling Uptake')\n",
    "ax1.legend(title='Cycling' ,\n",
    "           loc='center', \n",
    "           bbox_to_anchor=(-0.15,0.4), \n",
    "           labelspacing = 0.75, \n",
    "           frameon=False)\n",
    "ax2.legend(title='Population', \n",
    "           loc='center', \n",
    "           bbox_to_anchor=(-0.15,0.1), \n",
    "           labelspacing = 0.75, \n",
    "           frameon=False)\n",
    "\n",
    "ax1.grid(False)\n",
    "ax2.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# London shows strong decline which is due to the fact counting was stopped in 2020 for long periods.\n",
    "# This was seen is the calendar heatmap for missing data.\n",
    "# New York Cycling uptake peaked in 2016 and has been declining very slowly since.\n",
    "# London uptake (bar 2020) also very stable except good growth in the first few years.\n",
    "# Sydney uptake has been on a secular decline.\n",
    "# 2020/21 decline in Sydney possibly related to a change in count methodology.\n",
    "# There was a sharp decline in frequency of count data in 2020/21 in Sydney.\n",
    "# This was seen in the missing data distribution analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267becf5",
   "metadata": {},
   "source": [
    "## Comparative Analysis between London's regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bd5cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise change in uptake in the 3 parts of London over time.\n",
    "g=sns.lineplot(x=\"year\", \n",
    "               y=\"log_total_uptake_ldn\", \n",
    "               data=df1, \n",
    "               hue='functional_cycling_area', ci=None)\n",
    "g.set_title('Cycling uptake in London & its regions')\n",
    "g.set_ylabel('Cycling Uptake (log scale)')\n",
    "g.set_xlabel('Year')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(-0.15, 0.1), \n",
    "           borderaxespad=0, \n",
    "           frameon=False)\n",
    "\n",
    "plt.grid(b=None)\n",
    "\n",
    "# Central London uptake stable although slowly rising beyond pre-pandemic peak.\n",
    "# Strong growth in uptake between 2019 and 2020 in the regions.\n",
    "# Similar but less muted trend in Central London.\n",
    "# Sharp decline in regional uptake in the post pandemic period.\n",
    "# Could it mean leisure cyclists returning back to work and not cycling any longer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b6c177",
   "metadata": {},
   "source": [
    "## Macro view of London's total cycling Uptake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55d9dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the change in all areas of London combined.\n",
    "# Visualising on its own to manage scalar effects more tightly. \n",
    "# London is city of most interest.\n",
    "# Worth look at it on its own and also versus other cities.\n",
    "# Create the line chart to visualise the change.\n",
    "plt.plot(df['year'], df['log_total_uptake_ldn'])\n",
    "\n",
    "# Adding the aesthetics.\n",
    "plt.title(\"YoY cycling uptake in London across all regions, 2014-2021\")\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Cycling uptake (log scale)')\n",
    "plt.grid(None)\n",
    "\n",
    "# Show the plot.\n",
    "plt.show()\n",
    "\n",
    "# Strong growth from 2014 onwards every year although slows down dramatically in 2019.\n",
    "# Big decline in counts in 2019-2020 due to Covid-19 when lockdowns were in force.\n",
    "# Decline not due necessarily due to actual reduction in cyclists.\n",
    "# Counting had stopped through most of 2019-20 as displayed previously in missing data analysis.\n",
    "# Post pandemic recovery in numbers still below 2015.\n",
    "# Although 2021 numbers are not for full year, they do go up to end of first week of December 2021.\n",
    "# Shows that there is certainly room to increase cycling uptake from current levels.\n",
    "# Although expect that cycling counts should naturally creep up as post Covid normalisation continues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10754ac",
   "metadata": {},
   "source": [
    "## Micro Analysis of trends in London"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa775440",
   "metadata": {},
   "source": [
    "### Cycling Trends in London by month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd53d3",
   "metadata": {},
   "source": [
    "Pre-Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9233ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering Pre-Covid Time period.\n",
    "df1_precov = df1[(df1['year'] <2020)]\n",
    "\n",
    "# Sorting according to the month, otherwise visualization allocates random postions to months.\n",
    "df1_precov = df1_precov.sort_values(by=['month'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2751ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise change in uptake in the 3 parts of London over time.\n",
    "plt.rcParams['figure.figsize']=12,5\n",
    "g=sns.lineplot(x=\"month_name\", y=\"log_total_uptake_ldn\", data=df1_precov, hue='functional_cycling_area', \n",
    "               ci=None)\n",
    "g.set_title('Cycling uptake in London & its regions')\n",
    "g.set_ylabel('Cycling Uptake (log scale)')\n",
    "g.set_xlabel('Month')\n",
    "plt.legend(bbox_to_anchor=(-0.15, 0.1), borderaxespad=0, frameon=False)\n",
    "\n",
    "plt.grid(b=None)\n",
    "\n",
    "\n",
    "# Can see peak months are summer in all three areas of London.\n",
    "# Inner & Outer London skewed by the fact most of the count takes place between Mar - Jul & Sep-Oct.\n",
    "# But even here can see increase in uptake going into the peak summer months between May and Jul."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e5225a",
   "metadata": {},
   "source": [
    "Cycling Trend during Covid (2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3666dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering Covid Time period (2020).\n",
    "df1_covid = df1[(df1['year'] == 2020)]\n",
    "\n",
    "# Sorting according to the month, otherwise visualization allocates random postions to months.\n",
    "df1_covid = df1_covid.sort_values(by=['month'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise change in uptake in the 3 parts of London over time.\n",
    "g=sns.lineplot(x=\"month_name\", y=\"log_total_uptake_ldn\", data=df1_covid, hue='functional_cycling_area', \n",
    "               ci=None)\n",
    "g.set_title('Cycling uptake in London & its regions')\n",
    "g.set_ylabel('Cycling Uptake (log scale)')\n",
    "g.set_xlabel('Month')\n",
    "plt.legend(bbox_to_anchor=(-0.15, 0.1), borderaxespad=0, frameon=False)\n",
    "plt.rcParams['figure.figsize']=(12,5)\n",
    "plt.grid(b=None)\n",
    "\n",
    "# Although monthly patterns from Pre-Covid holds.\n",
    "# Trend here is likely linked to lock down periods.\n",
    "# Can clearly see that there is very little data for Inner & Central London in 2020.\n",
    "# Not very insightful for regional data.\n",
    "# London needs to automate or improve its count methodology.\n",
    "# How can New York count through COVID and not London??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba6bd7",
   "metadata": {},
   "source": [
    "Cycling Trend during Post-Covid (2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e76ebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering Covid Time period (2021).\n",
    "df1_postcovid = df1[(df1['year'] == 2021)]\n",
    "\n",
    "# Sorting values according to the month.\n",
    "df1_postcovid = df1_postcovid.sort_values(by=['month'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise change in uptake in the 3 parts of London over time.\n",
    "g=sns.lineplot(x=\"month_name\", y=\"log_total_uptake_ldn\", data=df1_postcovid, hue='functional_cycling_area', \n",
    "               ci=None)\n",
    "g.set_title('Cycling uptake in London & its regions')\n",
    "g.set_ylabel('Cycling Uptake (log scale)')\n",
    "g.set_xlabel('Month')\n",
    "plt.legend(bbox_to_anchor=(-0.15, 0.1), borderaxespad=0, frameon=False)\n",
    "plt.rcParams['figure.figsize']=(12,5)\n",
    "plt.grid(b=None)\n",
    "\n",
    "# Data in Outer & Inner London still very patchy.\n",
    "# No clear patterns here sas data is too patchy in the regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf3fbb",
   "metadata": {},
   "source": [
    "### Cycling Trends in London by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97fdcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by seasons to observe any patterns.\n",
    "df1_seasons = df1.groupby([\"season\"], as_index=False)['total_uptake'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a treemap.\n",
    "# Colour Palette.\n",
    "colors=['#fae588','#f79d65','#f9dc5c','#e8ac65','#e76f51','#ef233c','#b7094c']\n",
    "\n",
    "# Set Plot Style.\n",
    "sns.set_style(style=\"whitegrid\") \n",
    "\n",
    "# Setting the variable.\n",
    "sizes= df1_seasons[\"total_uptake\"].values\n",
    "label=df1_seasons[\"season\"]\n",
    "\n",
    "# Setting the styles.\n",
    "squarify.plot(sizes=sizes, label=label, alpha=0.6,color=colors).set(title='Cycling trends in London by season')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# This will be somewhat skewed given Outer & Inner London data is\\\n",
    "# counted in just the Spring & Summer months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1177af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting an alternative visualisation.\n",
    "plt.rcParams[\"axes.facecolor\"] = \"w\"\n",
    "sns.lineplot(data=df1_seasons, x=\"season\", y=\"total_uptake\", color=\"blue\", markers=True, \n",
    "            ci=None)\n",
    "plt.title(\"Cycling uptake in London by season\")\n",
    "plt.xlabel(\"Season\")\n",
    "plt.ylabel(\"Cycling uptake (log scale)\")\n",
    "plt.grid(None)\n",
    "plt.show()\n",
    "\n",
    "# Spring & Summer are peak 'seasons' for cycling in London.\n",
    "# Cycling craters in the winter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4757fb",
   "metadata": {},
   "source": [
    "### Cycling trends in London by weather & season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b629f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by weather type and months to observe any patterns.\n",
    "df1_seasons = df1.groupby([\"season\", \"weather\"], as_index=False)['total_uptake'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5e2335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control figure size for this notebook.\n",
    "plt.rcParams['figure.figsize'] = [8, 8]\n",
    "\n",
    "# Data.\n",
    "data = df1_seasons\n",
    " \n",
    "# Use the scatterplot function to build the bubble map.\n",
    "ax = sns.scatterplot(data=data, \n",
    "                x=\"season\", \n",
    "                y=\"weather\", \n",
    "                size=\"total_uptake\", \n",
    "                legend=False, \n",
    "                sizes=(30, 1000))\n",
    "\n",
    "ax.set_xlabel(\"Seasons\")\n",
    "ax.set_ylabel(\"Weather\")\n",
    "ax.set_title(\"Cycling trends in London during different weather conditions\")\n",
    "\n",
    "# Show the graph.\n",
    "plt.show()\n",
    "\n",
    "# Size of the bubbles indicates relative cycling uptake in each weather & season.\n",
    "# Clear to see that irrespective Season, cycling uptake is highest during periods of 'good' weather."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9607e2d",
   "metadata": {},
   "source": [
    "### Cycling trend in London by Period of the Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f0309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for assigning rank to Period of the day for sorting.\n",
    "def f(x):\n",
    "     if (x == \"Early Morning (06:00-07:00)\"):\n",
    "            return '0'\n",
    "    \n",
    "     elif (x == \"AM peak (07:00-10:00)\"):\n",
    "            return '1'\n",
    "    \n",
    "     elif (x == \"Inter-peak (10:00-16:00)\"):\n",
    "            return'2'\n",
    "    \n",
    "     elif (x == \"PM peak (16:00-19:00)\"):\n",
    "            return '3'\n",
    "    \n",
    "     elif (x == \"Evening (19:00-22:00)\"):\n",
    "            return'4'\n",
    "\n",
    "# Apply user defined function to create new column with seasons.\n",
    "df1['period_rank'] = df1['period'].apply(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aeebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting number of cycles into thousands for feasible graph comprehension.\n",
    "df1['total_uptake'] = df1['total_uptake']/1000\n",
    "\n",
    "# Sorting the values according to period rank.\n",
    "# Otherwise Periods will appear at random on the graph.\n",
    "df1 = df1.sort_values(by='period_rank', ascending=True)\n",
    "\n",
    "ax = sns.catplot(data=df1, kind=\"bar\", x= \"total_uptake\",y= \"period\", alpha=0.9, legend=True,\n",
    "                 palette='dark', ci = None, hue = \"functional_cycling_area\", \n",
    "                height=6)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.title(\"London cycling trends for period of the day (2015 - 2021)\")\n",
    "plt.xlabel(\"No. of cycles ( '000s )\")\n",
    "plt.ylabel(\"Period of the Day\")\n",
    "plt.xticks(rotation = 360)\n",
    "plt.grid(None)\n",
    "plt.show()\n",
    "\n",
    "# Patterns across all 3 regions the same.\n",
    "# AM Peak and PM Peak are the busiest periods.\n",
    "# In Outer London Inter-peak is also another busy period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2084d006",
   "metadata": {},
   "source": [
    "### Cycling trend in London by Day of the Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58519dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for assigning rank to:\n",
    "\n",
    "def f(x):\n",
    "     if (x == \"Monday\"):\n",
    "            return '1'\n",
    "    \n",
    "     elif (x == \"Tuesday\"):\n",
    "            return '2'\n",
    "    \n",
    "     elif (x == \"Wednesday\"):\n",
    "            return'3'\n",
    "    \n",
    "     elif (x == \"Thursday\"):\n",
    "            return '4'\n",
    "    \n",
    "     elif (x == \"Friday\"):\n",
    "            return'5'\n",
    "\n",
    "     elif (x == \"Saturday\"):\n",
    "            return '6'\n",
    "        \n",
    "     elif (x == \"Sunday\"):\n",
    "            return '7'\n",
    "        \n",
    "# Apply user defined function to create new column with day ranking (0 - 6).\n",
    "df1['day_rank'] = df1['day_of_week'].apply(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e1258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the values according to period rank.\n",
    "# Otherwise Periods will appear at random on the graph.\n",
    "df1 = df1.sort_values(by='day_rank', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26d84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting number of cycles into thousands for feasible graph comprehension.\n",
    "df1['total_uptake'] = df1['total_uptake']/1000\n",
    "\n",
    "# Setting out the plot.\n",
    "ax = sns.catplot(data=df1, kind=\"bar\", x= \"total_uptake\",y= \"day_of_week\", alpha=0.9, legend=True,\n",
    "                 palette='dark', ci = None, hue = \"functional_cycling_area\", \n",
    "                height=6)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.title(\"London cycling trends for day of the week (2015 - 2021)\")\n",
    "plt.xlabel(\"No. of cycles ( '000s )\")\n",
    "plt.ylabel(\"Day of the Week\")\n",
    "plt.xticks(rotation = 360)\n",
    "plt.show()\n",
    "\n",
    "# Cycling in Central London declines progressively through the week.\n",
    "# Tues-Thurs are peak cycling days in Outer & Inner London.\n",
    "# Almost no weekend cycling!\n",
    "# No cycling on Sundays in Outer London at all??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0743914c",
   "metadata": {},
   "source": [
    "### Cycling Trends in London by Ownership Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5841458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group ownership.\n",
    "df = london_complete.groupby([\"year\", \n",
    "                              \"functional_cycling_area\"], \n",
    "                              as_index=False)[\"number_of_private_cycles\",\n",
    "                                              \"number_of_cycle_hire_bikes\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960dc755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming Columns for easy comprehension and visualizations.\n",
    "df = df.rename(columns={'number_of_private_cycles': 'Private', \n",
    "                        'number_of_cycle_hire_bikes': 'Hired'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f96521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outer as functional cycling area as the area does not have any ownership data.\n",
    "df1 = df[df[\"functional_cycling_area\"].str.contains(\"Central|Inner\") == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839d14ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt dataframe to get Private and Hired ownership in one column.\n",
    "df1_melt = df1.melt(id_vars=[\"year\", \"functional_cycling_area\"],\n",
    "                              var_name=\"ownership\",\n",
    "                              value_name=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce2278",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set out the visualisation.\n",
    "g = sns.catplot(x=\"year\", \n",
    "                y=\"count\", \n",
    "                hue=\"ownership\", \n",
    "                kind = \"bar\", \n",
    "                col=\"functional_cycling_area\", \n",
    "                data=df1_melt)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Private ownership overwhelmingly accounts for both absolute count & growth in count\n",
    "# Almost no uptake in hired bikes in Inner\n",
    "# Hired bikes is a big area of opportunity to increase cycling uptake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b74a4",
   "metadata": {},
   "source": [
    "### Cycling Trends in London by Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5806662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group year and cycling count by gender.\n",
    "# Gender & cycling count data is in london_complete.\n",
    "df = london_complete.groupby([\"year\"], \n",
    "                             as_index=False)[\"number_of_male_cycles\",\n",
    "                                             \"number_of_female_cycles\", \n",
    "                                             \"number_of_unknown_cycles\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e95b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Renaming Columns for easy comprehension and visualizations.\n",
    "df = df.rename(columns={'number_of_male_cycles': 'Male', \n",
    "                        'number_of_female_cycles': 'Female',\n",
    "                        'number_of_unknown_cycles': 'Unknown'})\n",
    "\n",
    "# Add the totals males and females.\n",
    "df.loc['total'] = df.sum()\n",
    "\n",
    "# View output.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044209b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip the decimal point from the year column.\n",
    "df['year'] = df['year'].astype(str).str[:-2].astype(np.int64)\n",
    "df\n",
    "\n",
    "# Can see Males are 84% of total. \n",
    "# Females are 15% of total.\n",
    "# Unknown 1% of total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34056cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentages.\n",
    "total = 501584 + 90401 + 5198\n",
    "male = 501584\n",
    "female = 90401\n",
    "unknown = 5198\n",
    "\n",
    "percent_male = (male/total)*100\n",
    "percent_female = (female/total)*100\n",
    "percent_unknown = (unknown/total)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e6177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Waffle Chart.\n",
    "# Manually calculated % age of total from above dataframe.\n",
    "data = {'Male': (percent_male), 'Female': (percent_female), 'Unknown': (percent_unknown)}\n",
    "fig = plt.figure(FigureClass = Waffle, \n",
    "                 rows = 5, \n",
    "                 columns = 15, \n",
    "                 values = data, \n",
    "                 title = {'label': 'London Cyclists by Gender'}, \n",
    "                 legend={'loc': 'upper left', 'bbox_to_anchor': (1, 1)}, \n",
    "                 figsize = (10,15))\n",
    "\n",
    "# Data is incomplete.\n",
    "# Gender segregation only represented in Outer London Dataset.\n",
    "# How do the counters know gender?\n",
    "# Huge disparity in gender is very noteworth nevertheless!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539f5803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the total row.\n",
    "df.drop(df.tail(1).index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a1dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating change in gender distribution Y-o-Y.\n",
    "# Meling to get both genders in single column.\n",
    "df_melt = df.melt(id_vars=[\"year\"], \n",
    "                  var_name=\"gender\", \n",
    "                  value_name=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99b04e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See % change in counts amongst genders.\n",
    "df_melt['pct_yoy'] = df_melt['count'].pct_change()*100\n",
    "\n",
    "# View output randomly.\n",
    "df_melt.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2925639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with 2014 data.\n",
    "# Filtering Pre-Covid Time period.\n",
    "df1 = df_melt[(df_melt['year'] >2014)]\n",
    "\n",
    "# Drop rows with gender unknown.\n",
    "df1 = df1[df1[\"gender\"].str.contains(\"Unknown\") == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a9f301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualise Year on Year % Change. \n",
    "g = sns.catplot(x=\"year\", y=\"pct_yoy\", kind = \"bar\", col=\"gender\", data=df1)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.show()\n",
    "\n",
    "# Very volatile Y-o-Y % change.\n",
    "# No clear insight except that post Covid recovery amongst female riders is much stronger.\n",
    "# More than double the rate of increase in male cyclists.\n",
    "# Need to note that % change in women is from a very low base. \n",
    "# The stronger than male equivalent recovery should be used as a momentum builder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b08d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise absolute numbers across the years.\n",
    "# Reducing the scalar effect.\n",
    "df_melt['count'] = df_melt['count']/1000\n",
    "\n",
    "# Setting out the plot.\n",
    "ax = sns.catplot(data=df_melt, kind=\"bar\", x= \"year\", y= \"count\", hue=\"gender\")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.title(\"Gender distribution of Outer London Cyclists (2015 - 2021)\")\n",
    "plt.ylabel(\"No. of cycles ( '000s )\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.xticks(rotation = 360)\n",
    "plt.show()\n",
    "\n",
    "# Once again confirms the massive gender gap in cycling uptake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec944ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycling Infrastructure data exists only for 2017 & 2018.\n",
    "# To compare linkage with infrastructure and gender drop all rows\\\n",
    "# for gender where year is not 2017 or 2018.\n",
    "# Gender & cycling count data located in london_complete.\n",
    "df = london_complete[(london_complete.year == 2017) | (london_complete.year == 2018)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde0c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out boroughs which collect data on gender. \n",
    "# Group  by year.  \n",
    "df = df.groupby([\"year\", \"borough\"], as_index=False)[\"number_of_male_cycles\",\n",
    "                                                     \"number_of_female_cycles\", \n",
    "                                                     \"number_of_unknown_cycles\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518e866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming Columns for easy comprehension and visualizations.\n",
    "df = df.rename(columns={'number_of_male_cycles': 'Male', \n",
    "                        'number_of_female_cycles': 'Female',\n",
    "                       'number_of_unknown_cycles': 'Unknown'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a13e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop boroughs which dont collect gender data.\n",
    "df = df[~(df[['Male','Female','Unknown']] == 0).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12969345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how many boroughs collect gender data.\n",
    "for col in df:\n",
    "  print(col,\": \", df[col].nunique())\n",
    "\n",
    "# 23 Boroughs in London collect gender data in these two years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns Male & Unknown.\n",
    "df.drop(['Male', 'Unknown'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f06b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set out the visualisation.\n",
    "# Shows how the number of female cyclists have evolved over the two years of interest in each borough.\n",
    "g = sns.catplot(x=\"Female\", \n",
    "                y=\"borough\", \n",
    "                hue=\"year\", \n",
    "                kind = \"bar\", \n",
    "                data=df)\n",
    "\n",
    "plt.title(\"Female Cyclists in London's Boroughs (2017-2018)\")\n",
    "plt.ylabel(\"Borough Name\")\n",
    "plt.xlabel(\"No. Female Cyclists\")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.show()\n",
    "\n",
    "# Can see basically very little notable increase in most boroughs. \n",
    "# Except Ealing, Richmond upon Thames & Waltham Forest.\n",
    "# In fact in some boroughs, numbers of female cyclists declined considerably.\n",
    "# Such as in Brent, Greenwich, Harringey, Hillingdon, Hounslow, Lewisham, Merton & Sutton.\n",
    "# Other boroughs where female cyclists are interestingly large are Kingston upon Thames & Newham.\n",
    "# Worth looking at all these boroughs more closely and how they correspond with various infrastructure\\ \n",
    "# and total infrastructure.\n",
    "# So out of 23 London boroughs which collect gender data, we look at the 13 more closely\\\n",
    "# Ealing, Richmond upon Thames, Waltham Forest, Brent, Greenwich, Harringey, Hillingdon,\\\n",
    "# Hounslow, Lewisham, Merton, Sutton, Kingston upon Thames & Newham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6189cfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Borough to get total number of female cyclist per borough.\n",
    "df = df.groupby([\"borough\"], as_index=False)[\"Female\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a74063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the top 10 Boroughs for female cyclists.\n",
    "df_largest_ten = df.nlargest(10,'Female')\n",
    "\n",
    "df_largest_ten.plot.barh(x=\"borough\", \n",
    "                         y=\"Female\", \n",
    "                         title=\"Top 10 Boroughs for Female Cyclists 2017-2018\", \n",
    "                         legend=False, \n",
    "                         xlabel='borough', \n",
    "                         ylabel='Female Cyclists')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c25f31",
   "metadata": {},
   "source": [
    "### Investigating effect of each cycling infrastructure asset on female uptake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca4f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the formatting of year column.\n",
    "cyc_lane_yr_bor_grpby['year'] = cyc_lane_yr_bor_grpby['year'].astype(str).str[:-2].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab38b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyc_lane_yr_bor_grpby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580fb9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain only 2017 & 2018 data in the cycling infrastructure data.\n",
    "cyc_lane_yr_bor_grpby = cyc_lane_yr_bor_grpby[(cyc_lane_yr_bor_grpby.year == 2017) | (cyc_lane_yr_bor_grpby.year == 2018)]\n",
    "rct_route_yr_bor_grpby = rct_route_yr_bor_grpby[(rct_route_yr_bor_grpby.year == 2017) | (rct_route_yr_bor_grpby.year == 2018)]\n",
    "signage_yr_bor_grpby = signage_yr_bor_grpby[(signage_yr_bor_grpby.year == 2017) | (signage_yr_bor_grpby.year == 2018)]\n",
    "signal_yr_bor_grpby = signal_yr_bor_grpby[(signal_yr_bor_grpby.year == 2017) | (signal_yr_bor_grpby.year == 2018)]\n",
    "crossing_yr_bor_grpby = crossing_yr_bor_grpby[(crossing_yr_bor_grpby.year == 2017) | (crossing_yr_bor_grpby.year == 2018)]\n",
    "cycle_park_yr_bor_grpby = cycle_park_yr_bor_grpby[(cycle_park_yr_bor_grpby.year == 2017) | (cycle_park_yr_bor_grpby.year == 2018)]\n",
    "rct_point_yr_bor_grpby = rct_point_yr_bor_grpby[(rct_point_yr_bor_grpby.year == 2017) | (rct_point_yr_bor_grpby.year == 2018)]\n",
    "traf_calm_yr_bor_grpby = traf_calm_yr_bor_grpby[(cyc_lane_yr_bor_grpby.year == 2017) | (traf_calm_yr_bor_grpby.year == 2018)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16966214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the female count data with cycling infrastructure.\n",
    "df1 = df.merge(cyc_lane_yr_bor_grpby[['year', 'borough', 'infra_sum']])\n",
    "df2 = df.merge(rct_route_yr_bor_grpby[['year', 'borough', 'infra_sum']])\n",
    "df3 = df.merge(signage_yr_bor_grpby[['year', 'borough', 'infra_sum']])\n",
    "df4 = df.merge(signal_yr_bor_grpby[['year', 'borough', 'infra_sum']])\n",
    "df5 = df.merge(crossing_yr_bor_grpby[['year', 'borough', 'infra_sum']])\n",
    "df6 = df.merge(cycle_park_yr_bor_grpby[['year', 'borough', 'infra_sum']])\n",
    "df7 = df.merge(rct_point_yr_bor_grpby[['year', 'borough', 'infra_sum']])\n",
    "df8 = df.merge(traf_calm_yr_bor_grpby[['year', 'borough', 'infra_sum']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd51f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by Borough and summing the two variables.\n",
    "df1 = df1.groupby([\"borough\"], as_index=False)[\"Female\",\"infra_sum\"].sum()\n",
    "df2 = df2.groupby([\"borough\"], as_index=False)[\"Female\",\"infra_sum\"].sum()\n",
    "df3 = df3.groupby([\"borough\"], as_index=False)[\"Female\",\"infra_sum\"].sum()\n",
    "df4 = df4.groupby([\"borough\"], as_index=False)[\"Female\",\"infra_sum\"].sum()\n",
    "df5 = df5.groupby([\"borough\"], as_index=False)[\"Female\",\"infra_sum\"].sum()\n",
    "df6 = df6.groupby([\"borough\"], as_index=False)[\"Female\",\"infra_sum\"].sum()\n",
    "df7 = df7.groupby([\"borough\"], as_index=False)[\"Female\",\"infra_sum\"].sum()\n",
    "df8 = df8.groupby([\"borough\"], as_index=False)[\"Female\",\"infra_sum\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292308be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising Cycle Lanes & Female Cycling Uptake by boroughs.\n",
    "\n",
    "# Use the scatterplot function to build the bubble map.\n",
    "\n",
    "sns.scatterplot(data=df1, \n",
    "                x=\"infra_sum\", \n",
    "                y=\"borough\", \n",
    "                size=\"Female\",\n",
    "                hue=\"Female\", \n",
    "                legend=True,\n",
    "                sizes=(20, 2000), \n",
    "                palette= \"tab10\"\n",
    "                )\n",
    "\n",
    "\n",
    "plt.title('Cycle Lanes & Female Cycling Uptake by boroughs')\n",
    "# Set x-axis label.\n",
    "plt.xlabel('Number of Cycle Lanes')\n",
    "# Set y-axis label.\n",
    "plt.ylabel('Buroughs')\n",
    "\n",
    "# Position of Legend.\n",
    "plt.legend(bbox_to_anchor=(1.25, 0.5), loc='center right', borderaxespad=0, title= 'Female Cyclists')\n",
    "\n",
    "# show the graph.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79161f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising Restricted Routes & Female Cycling Uptake by boroughs.\n",
    "\n",
    "# Use the scatterplot function to build the bubble map.\n",
    "\n",
    "sns.scatterplot(data=df2, \n",
    "                x=\"infra_sum\", \n",
    "                y=\"borough\", \n",
    "                size=\"Female\",\n",
    "                hue=\"Female\", \n",
    "                legend=True,\n",
    "                sizes=(20, 2000), \n",
    "                palette= \"tab10\"\n",
    "               )\n",
    "\n",
    "plt.title('Restricted Routes & Female Cycling Uptake by boroughs')\n",
    "# Set x-axis label.\n",
    "plt.xlabel('Number of Restricted Routes')\n",
    "# Set y-axis label.\n",
    "plt.ylabel('Buroughs')\n",
    "\n",
    "# Position of Legend.\n",
    "plt.legend(bbox_to_anchor=(1.25, 0.5), loc='center right', borderaxespad=0, title= 'Female Cyclists')\n",
    "\n",
    "# show the graph.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc1ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising Signage & Female Cycling Uptake by boroughs.\n",
    "\n",
    "# Use the scatterplot function to build the bubble map.\n",
    "\n",
    "sns.scatterplot(data=df3, \n",
    "                x=\"infra_sum\", \n",
    "                y=\"borough\", \n",
    "                size=\"Female\",\n",
    "                hue=\"Female\", \n",
    "                legend=True,\n",
    "                sizes=(20, 2000), \n",
    "                palette= \"tab10\"\n",
    "               )\n",
    "\n",
    "plt.title('Signage & Female Cycling Uptake by boroughs')\n",
    "# Set x-axis label.\n",
    "plt.xlabel('Number of Signages')\n",
    "# Set y-axis label.\n",
    "plt.ylabel('Buroughs')\n",
    "\n",
    "# Position of Legend.\n",
    "plt.legend(bbox_to_anchor=(1.25, 0.5), loc='center right', borderaxespad=0, title= 'Female Cyclists')\n",
    "\n",
    "# show the graph.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bb83f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising Signals & Female Cycling Uptake by boroughs.\n",
    "\n",
    "# Use the scatterplot function to build the bubble map.\n",
    "\n",
    "sns.scatterplot(data=df4, \n",
    "                x=\"infra_sum\", \n",
    "                y=\"borough\", \n",
    "                size=\"Female\",\n",
    "                hue=\"Female\", \n",
    "                legend=True,\n",
    "                sizes=(20, 2000), \n",
    "                palette= \"tab10\"\n",
    "               )\n",
    "\n",
    "plt.title('Signals & Female Cycling Uptake by boroughs')\n",
    "# Set x-axis label.\n",
    "plt.xlabel('Number of Signals')\n",
    "# Set y-axis label.\n",
    "plt.ylabel('Buroughs')\n",
    "\n",
    "# Position of Legend.\n",
    "plt.legend(bbox_to_anchor=(1.25, 0.5), loc='center right', borderaxespad=0, title= 'Female Cyclists')\n",
    "\n",
    "# show the graph.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e074ce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising Crossing & Female Cycling Uptake by boroughs.\n",
    "\n",
    "# Use the scatterplot function to build the bubble map.\n",
    "\n",
    "sns.scatterplot(data=df5, \n",
    "                x=\"infra_sum\", \n",
    "                y=\"borough\", \n",
    "                size=\"Female\",\n",
    "                hue=\"Female\", \n",
    "                legend=True,\n",
    "                sizes=(20, 2000), \n",
    "                palette= \"tab10\"\n",
    "               )\n",
    "\n",
    "plt.title('Cycle Crossings & Female Cycling Uptake by boroughs')\n",
    "# Set x-axis label.\n",
    "plt.xlabel('Number of Crossings')\n",
    "# Set y-axis label.\n",
    "plt.ylabel('Buroughs')\n",
    "\n",
    "# Position of Legend.\n",
    "plt.legend(bbox_to_anchor=(1.25, 0.5), loc='center right', borderaxespad=0, title= 'Female Cyclists')\n",
    "\n",
    "# show the graph.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93072109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising Cycle Parkings & Female Cycling Uptake by boroughs.\n",
    "\n",
    "# Use the scatterplot function to build the bubble map.\n",
    "\n",
    "sns.scatterplot(data=df6, \n",
    "                x=\"infra_sum\", \n",
    "                y=\"borough\", \n",
    "                size=\"Female\",\n",
    "                hue=\"Female\", \n",
    "                legend=True,\n",
    "                sizes=(20, 2000), \n",
    "                palette= \"tab10\"\n",
    "               )\n",
    "\n",
    "plt.title('Cycle Parking & Female Cycling Uptake by boroughs')\n",
    "# Set x-axis label.\n",
    "plt.xlabel('Number of Cycle Parkings')\n",
    "# Set y-axis label.\n",
    "plt.ylabel('Buroughs')\n",
    "\n",
    "# Position of Legend.\n",
    "plt.legend(bbox_to_anchor=(1.25, 0.5), loc='center right', borderaxespad=0, title= 'Female Cyclists')\n",
    "\n",
    "# Show the graph.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de634a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising Restricted Points & Female Cycling Uptake by boroughs.\n",
    "\n",
    "# Use the scatterplot function to build the bubble map.\n",
    "\n",
    "sns.scatterplot(data=df7, \n",
    "                x=\"infra_sum\", \n",
    "                y=\"borough\", \n",
    "                size=\"Female\",\n",
    "                hue=\"Female\", \n",
    "                legend=True,\n",
    "                sizes=(20, 2000), \n",
    "                palette= \"tab10\"\n",
    "               )\n",
    "\n",
    "plt.title('Restricted Points & Female Cycling Uptake by boroughs')\n",
    "# Set x-axis label.\n",
    "plt.xlabel('Restricted Points')\n",
    "# Set y-axis label.\n",
    "plt.ylabel('Buroughs')\n",
    "\n",
    "# Position of Legend.\n",
    "plt.legend(bbox_to_anchor=(1.25, 0.5), loc='center right', borderaxespad=0, title= 'Female Cyclists')\n",
    "\n",
    "# show the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising Traffic Calming & Female Cycling Uptake by boroughs.\n",
    "\n",
    "# Use the scatterplot function to build the bubble map.\n",
    "\n",
    "sns.scatterplot(data=df8, x=\"infra_sum\", \n",
    "                y=\"borough\", \n",
    "                size=\"Female\",\n",
    "                hue=\"Female\", \n",
    "                legend=True,\n",
    "                sizes=(20, 2000), \n",
    "                palette= \"tab10\"\n",
    "               )\n",
    "\n",
    "plt.title('Traffic Calming & Female Cycling Uptake by boroughs')\n",
    "# Set x-axis label.\n",
    "plt.xlabel('Traffic Calming')\n",
    "# Set y-axis label.\n",
    "plt.ylabel('Buroughs')\n",
    "\n",
    "# Position of Legend.\n",
    "plt.legend(bbox_to_anchor=(1.25, 0.5), loc='center right', borderaxespad=0, title= 'Female Cyclists')\n",
    "\n",
    "# show the graph.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec47bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Visualising Restricted Routes & Female Cycling Uptake by boroughs\n",
    "# Will use two horizontal bar charts side by side on a shared axis\n",
    "\n",
    "# Flipping the values of the two variables to plot on shared axis\n",
    "\n",
    "\n",
    "# Setting up an ordering for the shared axis\n",
    "# Descending order set for boroughs vis a vis Total Infrastructure\n",
    "boroughs = ['Croydon', 'Waltham Forest', 'Lambeth', 'Greenwich', 'Ealing', \n",
    "            'Barking & Dagenham', 'Enfield', 'Newham', 'Hounslow', 'Richmond upon Thames', \n",
    "            'Hillingdon', 'Harrow', 'Kingston upon Thames', 'Merton', 'Redbridge', \n",
    "            'Sutton', 'Haringey', 'Bromley', 'Havering', 'Brent', 'Lewisham', \n",
    "            'Bexley', 'Barnet']\n",
    "\n",
    "# Setting up the plot area\n",
    "sns.set_style(style='white')\n",
    "\n",
    "# Setting up the two horizontal bar plots\n",
    "ax1 = sns.barplot(x='infra_sum', y='borough', data=df2, order=boroughs, palette=\"Set2\")\n",
    "ax2 = sns.barplot(x='Female', y='borough', data=df2, order=boroughs, palette=\"Set2\")\n",
    "plt.title(\"Impact of Restricted Routes on Female Cycling Uptake in London 2017-2018\")\n",
    "plt.xlabel(\"               Female Cycling Uptake      /      Restricted Routes\")\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Restricted Routes dont matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0dd2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating all cycling infrastructure data.\n",
    "infra_complete=pd.concat([df1, df2, df3, df4, df5, df6, df7, df8])\n",
    "\n",
    "# Adding up the total infrastructure in each borough of interest.\n",
    "infra_complete1 = infra_complete.groupby([\"borough\"], \n",
    "                                         as_index=False)[\"infra_sum\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77427a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the top 10 Boroughs for total infrastructure.\n",
    "infra_largest_ten = infra_complete1.nlargest(10,'infra_sum')\n",
    "\n",
    "infra_largest_ten.plot.barh(x=\"borough\", \n",
    "                            y=\"infra_sum\", \n",
    "                            title=\"Top 10 Boroughs with Female Cyclists for Total Cycle Infrastructure 2017-2018\", \n",
    "                            legend=False, \n",
    "                            xlabel='Total Infrastructure', \n",
    "                            ylabel='Borough Name' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Total Infrastructure with Female cyclist counts.\n",
    "dfs = [infra_complete1,df]\n",
    "on = ['borough']\n",
    "gender_infra = ft.reduce(lambda left, right: pd.merge(left, right, on=on), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784109e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by Female Cyclists.\n",
    "gender_infra = gender_infra.sort_values('Female')\n",
    "\n",
    "# Strip decimal points.\n",
    "gender_infra['Female'] = gender_infra['Female'].astype(str).str[:-2].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac766e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising Total Infrastructure of each Borough & Female Cycling Uptake.\n",
    "\n",
    "# Use the scatterplot function to build the bubble map.\n",
    "\n",
    "sns.scatterplot(data=df8, x=\"infra_sum\", \n",
    "                y=\"borough\", \n",
    "                size=\"Female\",\n",
    "                hue=\"Female\", \n",
    "                legend=True,\n",
    "                sizes=(20, 2000) \n",
    "                #palette= \"black\"\n",
    "               )\n",
    "\n",
    "plt.title('Total units of avaiable infrastructure assets', fontsize=23)\n",
    "# Set x-axis label.\n",
    "plt.xlabel('Total cycling infrastructure available in London', fontsize=18)\n",
    "# Set y-axis label.\n",
    "plt.ylabel('Boroughs', fontsize=18)\n",
    "\n",
    "# Position of Legend.\n",
    "plt.legend(bbox_to_anchor=(1.25, 0.5), loc='center right', borderaxespad=0, title= 'Female Cyclists')\n",
    "\n",
    "# Show the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0973e9",
   "metadata": {},
   "source": [
    "## Hired Bikes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f21b686",
   "metadata": {},
   "source": [
    "This analysis is conducted to reveal insights through cycle hiring patterns and align strategy on cycling-specific infrastructure connectivity (hire bikes to broader transport network and reach total transport strategy - last mile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd300d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dataset.\n",
    "cycle_hire = pd.read_csv('TFL Cycle Hire 2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec1114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping dataset according to routes taken by cyclists.\n",
    "cycle_hire_routes = cycle_hire.groupby(['StartStation Name', \n",
    "                                        'EndStation Name']).size().reset_index(name='Freq')\n",
    "\n",
    "# Adding a column to allocate serial number to the routes.\n",
    "cycle_hire_routes.insert(0, '#', range(1, 1 + len(cycle_hire_routes)))\n",
    "\n",
    "# Calculating percetages of Frequecies of the routes\n",
    "cycle_hire_routes['Freq %'] = (cycle_hire_routes['Freq'] / cycle_hire_routes['Freq'].sum()) * 100\n",
    "\n",
    "\n",
    "# Sorting Dataset according to most frequented routes.\n",
    "cycle_hire_routes = cycle_hire_routes.sort_values(by='Freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f3502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing number of Unique start stations.\n",
    "cycle_hire_start_stations = cycle_hire_routes['StartStation Name'].unique()\n",
    "\n",
    "# Printing the names of Unique start stations.\n",
    "print(cycle_hire_start_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dea5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing number of Unique start stations.\n",
    "cycle_hire_end_stations = cycle_hire_routes['EndStation Name'].unique()\n",
    "\n",
    "# Printing the names of Unique start stations.\n",
    "print(cycle_hire_end_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478788b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing ten most frequented Start Stations for Hired Bikes.\n",
    "\n",
    "cycle_hire_start_stations_ten_largest = cycle_hire_routes.nlargest(10,'Freq')\n",
    "\n",
    "cycle_hire_start_stations_ten_largest.plot.barh(x=\"StartStation Name\", \n",
    "                                                y=\"Freq\", \n",
    "                                                title=\"Top 10 Start Stations for Hired Bikes\", \n",
    "                                                legend=False, \n",
    "                                                xlabel='Start Stations' , \n",
    "                                                ylabel='Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c1ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing ten most frequented Start Stations for Hired Bikes.\n",
    "\n",
    "cycle_hire_end_stations_ten_largest = cycle_hire_routes.nlargest(10,'Freq')\n",
    "\n",
    "cycle_hire_end_stations_ten_largest.plot.barh(x=\"EndStation Name\", \n",
    "                                              y=\"Freq\", \n",
    "                                              title=\"Top 10 end Stations for Hired Bikes\", \n",
    "                                              legend=False, \n",
    "                                              xlabel='End Stations', \n",
    "                                              ylabel='No. of Trips')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496a7e3c",
   "metadata": {},
   "source": [
    "## Using Heatmaps & Geospatial data to display time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c1b9d",
   "metadata": {},
   "source": [
    "This section is intended to be technology demonstrator. If successful, the team will use this technology to build out key presentation topics for the final submission. For now, this is exploratory. Many of the varaibles were explored using this technology. For the purpose of this workbook one or two examples are being included by way of augmenting the analysis here as well as demonstrating the process and code behind the analysis using this tool which creates stunning visuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888b0acb",
   "metadata": {},
   "source": [
    "### Average number of cyclists throughout the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7bfd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_locations = london_complete[[\"site_id\",\"location\",\"latitude\",\"longitude\"]]\n",
    "london_locations = london_locations.drop_duplicates()\n",
    "\n",
    "london_period2 = london_complete.groupby([\"site_id\", \"period\",\"time\"]).agg(\"sum\").reset_index()\n",
    "london_period2 = london_period2.sort_values(by=\"time\",ascending=True)\n",
    "london_period2 = london_period2[[\"site_id\",\"period\",\"total_uptake\"]]\n",
    "\n",
    "london_period2_data = pd.merge(london_period2,london_locations,how=\"left\",on=\"site_id\")\n",
    "\n",
    "fig = px.density_mapbox(london_period2_data,\n",
    "                        lat='latitude', \n",
    "                        lon='longitude', \n",
    "                        z='total_uptake', \n",
    "                        radius=10,\n",
    "                        center=dict(lat=0, \n",
    "                                    lon=180), \n",
    "                        animation_frame=\"period\",\n",
    "                        zoom=0,\n",
    "                        hover_name=\"location\",\n",
    "                        hover_data={\"site_id\":False,\n",
    "                                    \"period\":True,\n",
    "                                    \"total_uptake\":True,\n",
    "                                   \"location\":True,\n",
    "                                   \"latitude\":False,\n",
    "                                   \"longitude\":False},\n",
    "                                      title=\"Average number of cyclists throughout different times of the day\",\n",
    "                                      height=1000,\n",
    "                                      width=1000,\n",
    "                                      opacity=1,\n",
    "                                      mapbox_style=\"stamen-terrain\")\n",
    "fig.show()\n",
    "\n",
    "# This shows the average flow of cyclists in all areas of London at any given time of the day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e532046",
   "metadata": {},
   "source": [
    "### Evolution of Cyclist Counts in London's Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9a9da",
   "metadata": {},
   "source": [
    "**Inner London**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0295f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_locations = london_complete[[\"site_id\",\"location\",\"latitude\",\"longitude\"]]\n",
    "london_locations = london_locations.drop_duplicates()\n",
    "\n",
    "london_areas = london_complete.groupby([\"year\", \"site_id\", \"functional_cycling_area\"]).agg(\"sum\").reset_index()\n",
    "london_areas = london_areas[[\"year\",\"site_id\",\"functional_cycling_area\",\"total_uptake\"]]\n",
    "\n",
    "london_area_data = pd.merge(london_areas,london_locations,how=\"left\",on=\"site_id\")\n",
    "\n",
    "fig = px.density_mapbox(london_area_data[london_area_data[\"functional_cycling_area\"]==\"Inner\"],\n",
    "                        lat='latitude', \n",
    "                        lon='longitude', \n",
    "                        z='total_uptake', \n",
    "                        radius=10,\n",
    "                        center=dict(lat=0, \n",
    "                                    lon=180), \n",
    "                        animation_frame=\"year\",\n",
    "                        zoom=0,\n",
    "                        hover_name=\"location\",\n",
    "                        hover_data={\"year\":True,\n",
    "                                    \"site_id\":False,\n",
    "                                    \"functional_cycling_area\":True,\n",
    "                                    \"total_uptake\":True,\n",
    "                                   \"location\":True,\n",
    "                                   \"latitude\":False,\n",
    "                                   \"longitude\":False},\n",
    "                                      title=\"Year-On-Year flow of Cyclist Counts in Inner london (2015-2021)\",\n",
    "                                      height=1000,\n",
    "                                      width=1000,\n",
    "                                      opacity=1,\n",
    "                                      mapbox_style=\"stamen-terrain\")\n",
    "fig.show()\n",
    "\n",
    "# Not all areas retain the same amount of cyclist count through the period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b83b45",
   "metadata": {},
   "source": [
    "**Central London**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d628cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.density_mapbox(london_area_data[london_area_data[\"functional_cycling_area\"]==\"Central\"],\n",
    "                        lat='latitude', \n",
    "                        lon='longitude', \n",
    "                        z='total_uptake', \n",
    "                        radius=10,\n",
    "                        center=dict(lat=0, \n",
    "                                    lon=180), \n",
    "                        animation_frame=\"year\",\n",
    "                        zoom=0,\n",
    "                        hover_name=\"location\",\n",
    "                        hover_data={\"year\":True,\n",
    "                                    \"site_id\":False,\n",
    "                                    \"functional_cycling_area\":True,\n",
    "                                    \"total_uptake\":True,\n",
    "                                   \"location\":True,\n",
    "                                   \"latitude\":False,\n",
    "                                   \"longitude\":False},\n",
    "                                      title=\"Year-On-Year flow of Cyclist Counts in Central London (2014-2021)\",\n",
    "                                      height=1000,\n",
    "                                      width=1000,\n",
    "                                      opacity=1,\n",
    "                                      mapbox_style=\"stamen-terrain\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458c09b6",
   "metadata": {},
   "source": [
    "**This format is very successful. However, given that these heatmaps are very resource intensive, the remainder of this work for the final submission will be switched to Tableau where the build-up & UX are more visually compelling whilst all of the analysis technology is retained.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739ee58",
   "metadata": {},
   "source": [
    "##### Quantitative Analysis\n",
    "\n",
    "Although the data set is relatively small with just over 195 rows of data, it is worth running some simple quantitative analysis to investigate if traffic flow and car ownership have any influence on each other and more importantly on cycling uptake. They could point towards longer term policy potentials to influence and increase cycling uptake in London."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757aa1b4",
   "metadata": {},
   "source": [
    "### Preparing the data for Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da936ede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting the shape of traffic flow, Car ownership and cycle count organised. \n",
    "# Melt the dataframe on car ownership.\n",
    "# This will align shape for further analysis.\n",
    "# will categorise the boroughs into central, inner and outer london values.\n",
    "df6 = pd.melt(car_own, id_vars =['year'], value_vars =['city_of_london', 'barking_and_dagenham', 'barnet', 'bexley', \n",
    "                                                      'brent', 'bromley', 'camden', 'croydon', 'ealing', 'enfield', \n",
    "                                                      'greenwich', 'hackney', 'hammersmith_and_fulham', 'haringey', \n",
    "                                                      'harrow', 'havering', 'hillingdon', 'hounslow', 'islington', \n",
    "                                                      'kensington_and_chelsea', 'kingston_upon_thames', 'lambeth', \n",
    "                                                      'lewisham', 'merton', 'newham', 'redbridge', 'richmond_upon_thames', \n",
    "                                                      'southwark', 'sutton', 'tower_hamlets', 'waltham_forest', \n",
    "                                                      'wandsworth', 'westminster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5de7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename variable to align to other datasets on london.\n",
    "df6 = df6.rename(columns={'variable': 'borough', 'value': 'car_ownership_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b946a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the dataframe on traffic flows.\n",
    "# This will align shape for further analysis.\n",
    "# Will categorise the boroughs into central, inner and outer london values.\n",
    "\n",
    "df7 = pd.melt(traffic_flow, id_vars =['year'], value_vars =['city_of_london', 'barking_and_dagenham', 'barnet', 'bexley', \n",
    "                                                      'brent', 'bromley', 'camden', 'croydon', 'ealing', 'enfield', \n",
    "                                                      'greenwich', 'hackney', 'hammersmith_and_fulham', 'haringey', \n",
    "                                                      'harrow', 'havering', 'hillingdon', 'hounslow', 'islington', \n",
    "                                                      'kensington_and_chelsea', 'kingston_upon_thames', 'lambeth', \n",
    "                                                      'lewisham', 'merton', 'newham', 'redbridge', 'richmond_upon_thames', \n",
    "                                                      'southwark', 'sutton', 'tower_hamlets', 'waltham_forest', \n",
    "                                                      'wandsworth', 'westminster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08628fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename variable to align to other datasets on london.\n",
    "df7 = df7.rename(columns={'variable': 'borough', 'value': 'traffic_flow'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8434ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the traffic flow and car ownership details.\n",
    "dfs = [df6, df7]\n",
    "cols= [\"year\", \"borough\"]\n",
    "other_stats = ft.reduce(lambda left, right: pd.merge(left, right, on=cols), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d972cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now manipulate cycle count data in london and add to above dataframe.\n",
    "# Reshape london df into a smaller df8 so its more resource efficient during manipulation.\n",
    "df8 = london_complete.drop(['latitude', 'longitude', 'survey_date', 'number_of_male_cycles', \n",
    "                            'number_of_female_cycles', 'number_of_unknown_cycles', 'day_of_week', \n",
    "                           'month', 'number_of_private_cycles', 'season', 'number_of_private_cycles', \n",
    "                           'number_of_cycle_hire_bikes', 'total_outer', 'total_inn_cen', 'weather', 'period'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f85f590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align name of boroughs across df8 so that it can be merged. \n",
    "# Change content of borough column into lower case. \n",
    "df8['borough'] = df8['borough'].str.lower()\n",
    "# Replace & with and\n",
    "df8.replace({'borough': '&'}, {'borough': 'and'}, regex=True, inplace = True)\n",
    "# Strip spaces with _\n",
    "df8.replace({'borough': ' '}, {'borough': '_'}, regex=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76ad8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by year and borough to align to traffic and car ownership count data.\n",
    "df8 = df8.groupby(['year', 'borough'], as_index=False)['total_uptake'].sum()\n",
    "df8.sort_values(['borough'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4884a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the traffic flow and car ownership details.\n",
    "dfs = [other_stats, df8]\n",
    "cols= [\"year\", \"borough\"]\n",
    "combined_stats_ldn = ft.reduce(lambda left, right: pd.merge(left, right, on=cols), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a6986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip non quantitative data columns.\n",
    "# Will use these to test for correlation amongst numeric variable.\n",
    "df10=combined_stats_ldn.drop(['year', 'borough'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename variable for clarity on visual analysis.\n",
    "df10 = df10.rename(columns={'car_ownership_count': 'car ownership', 'traffic_flow': 'traffic flow',\n",
    "                           'total_uptake': 'cycling uptake'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26dba6",
   "metadata": {},
   "source": [
    "### Conducting the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befe1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of each variable.\n",
    "# Setting up the correlation matrix.\n",
    "plt.figure\n",
    "sns.heatmap(df10.corr(),annot=True,cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix between Car Ownership, Traffic Flow & Cycling Uptake\", pad=20)\n",
    "plt.show()\n",
    "\n",
    "# Total traffic flow and car ownership in any location are strongly positively correlated.\n",
    "# Implies an increase in one will lead to an increase in the other. \n",
    "# Correlation is not causation and requires further investigation.\n",
    "# However, on the surface & intuitively these relationships make sense.\n",
    "# Also note similar relationships identified globally in PESTLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a57c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise sort of relationships if any between the numeric variablessns.set(style=\"ticks\", color_codes=True).\n",
    "sns.pairplot(df10)\n",
    "\n",
    "# Setting up adjustments to the plot area.\n",
    "plt.suptitle(\"Regression Test\")\n",
    "plt.subplots_adjust(top=0.9)\n",
    "\n",
    "# Can visualise a reasonable linear relationship between traffic flow and car ownership.\n",
    "# Visualisation doesnt suggest linear relationship between cycling up take and other variables.\n",
    "# So although correlation matrix suggest very strong correlation between all three variables.\n",
    "# Limited linear relationship established.\n",
    "# Given the strength of the inverse correlation between cycling uptake and car ownership and traffic flow.\n",
    "# Even though the relationship is not linear, car ownership and thus traffic flow should be targetted for reduction.\n",
    "# This will improve cycling uptake.\n",
    "# Evidence in PESTLE suggests non motorised zones in cities improves cycling uptake.\n",
    "# This is often the corner stone of government policies around increasing cycling uptake.\n",
    "# TW did not provide positive feedback on this work surprisingly.\n",
    "# So this potential area of long term interest and one that is being evidently pursued in other cities is being dropped.\n",
    "# This area of work can be picked up later and policy options explored using social analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362035fb",
   "metadata": {},
   "source": [
    "### Regression Analysis\n",
    "\n",
    "Will help quantify visualisations in the pairplots above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a2fb8b",
   "metadata": {},
   "source": [
    "Car Ownership & Traffic Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6fedc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Simple Linear Regression Model.\n",
    "# Dependent Variable.\n",
    "y = df10['traffic flow']\n",
    "\n",
    "# Independent Variable.\n",
    "x = df10['car ownership']\n",
    "\n",
    "# OLS model.\n",
    "f = 'y ~ x'\n",
    "test = ols(f, data=df10).fit()\n",
    "print(test.summary())\n",
    "\n",
    "# Pretty strong R2 suggesting 72% of changes in traffic flow are caused by changes in car ownership. \n",
    "# Prob F-stat is much smaller than the threshold which testifies that the variables are significant.\n",
    "# Large value F Statistic also suggests that the null hypothesis can be rejected.\n",
    "# p-value supports this.\n",
    "# Meaning we can hypothesise statistically that traffic flow and car ownership affect each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8a908",
   "metadata": {},
   "source": [
    "Car Ownership & Cycling uptake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f20853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a simple linear regression model.\n",
    "# Dependent Variable.\n",
    "y = df10['cycling uptake']\n",
    "\n",
    "# Independent Variable.\n",
    "x = df10['car ownership']\n",
    "\n",
    "# OLS model.\n",
    "f = 'y ~ x'\n",
    "test = ols(f, data=df10).fit()\n",
    "print(test.summary())\n",
    "\n",
    "# Much lower R2 which was to be expected. \n",
    "# Non linear relationship between the variables was previously visualised.\n",
    "# However F Statistic supports that a relationship exists between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205383d5",
   "metadata": {},
   "source": [
    "Traffic Flow & Cycling uptake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95abf669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a simple linear regression model.\n",
    "# Dependent Variable.\n",
    "y = df10['cycling uptake']\n",
    "\n",
    "# Independent Variable.\n",
    "x = df10['traffic flow']\n",
    "\n",
    "# OLS model.\n",
    "f = 'y ~ x'\n",
    "test = ols(f, data=df10).fit()\n",
    "print(test.summary())\n",
    "\n",
    "# Even lower R - squared but again output is expected.\n",
    "# A non linear relationship between the variables was previously visualised.\n",
    "# Not real world useful.\n",
    "# F Statistic still significant enough to suggest a relationship between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9248a13e",
   "metadata": {},
   "source": [
    "The quantitative analysis section is being dropped following lo-fi feedback from TW. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
